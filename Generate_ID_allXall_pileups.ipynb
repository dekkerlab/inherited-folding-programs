{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8a52a5-98ef-4c3f-a0b8-a6f1b41abe64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the number of threads for many common libraries\n",
    "from os import environ\n",
    "N_THREADS = '1'\n",
    "environ['OMP_NUM_THREADS'] = N_THREADS\n",
    "environ['OPENBLAS_NUM_THREADS'] = N_THREADS\n",
    "environ['MKL_NUM_THREADS'] = N_THREADS\n",
    "environ['VECLIB_MAXIMUM_THREADS'] = N_THREADS\n",
    "environ['NUMEXPR_NUM_THREADS'] = N_THREADS\n",
    "# https://superfastpython.com/numpy-number-blas-threads/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adeb24e-aa19-490c-bfc3-16b011e36a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%config InlineBackend.print_figure_kwargs={'bbox_inches':None}\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Hi-C utilities imports:\n",
    "import cooler\n",
    "import bioframe\n",
    "import cooltools\n",
    "# Visualization imports:\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm, Normalize\n",
    "from matplotlib import colors\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib.ticker import EngFormatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bb4552-d8be-4c72-b14e-c95bb44b12c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bbi for stackups ...\n",
    "import bbi\n",
    "# functions and assets specific to this repo/project ...\n",
    "from data_catalog import bws, bws_vlim, telo_dict\n",
    "from tqdm import tqdm\n",
    "from tqdm.notebook import trange, tqdm\n",
    "# import mpire for nested multi-processing\n",
    "from mpire import WorkerPool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e88790-01dc-4356-bc22-2e45ef0ce8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_mb(bp_val):\n",
    "    # check MB\n",
    "    if np.mod(bp_val, 1_000_000):\n",
    "        # just give 1 decimal if not even Mb\n",
    "        return f\"{bp_val/1_000_000:.1f}\"\n",
    "    else:\n",
    "        return f\"{bp_val//1_000_000}\"\n",
    "\n",
    "# given the range - generate pretty axis name\n",
    "def _get_name(_left, _right, _amount):\n",
    "    if np.isclose(_left, 0.0):\n",
    "        return f\"<{to_mb(_right)} Mb: {_amount}\"\n",
    "    elif _right > 80_000_000:\n",
    "        return f\">{to_mb(_left)} Mb: {_amount}\"\n",
    "    else:\n",
    "        return f\"{to_mb(_left)}-{to_mb(_right)} Mb: {_amount}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef645cb-8dec-414b-87cc-32a4cbd2981d",
   "metadata": {},
   "source": [
    "### Chrom arms as a view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5689161-6ec4-4d74-911d-70479f0d1372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use bioframe to fetch the genomic features from the UCSC.\n",
    "hg38_chromsizes = bioframe.fetch_chromsizes('hg38')\n",
    "hg38_cens = bioframe.fetch_centromeres('hg38')\n",
    "hg38_arms_full = bioframe.make_chromarms(hg38_chromsizes, hg38_cens)\n",
    "# # remove \"bad\" chromosomes and near-empty arms ...\n",
    "# excluded_arms = [\"chr13_p\", \"chr14_p\", \"chr15_p\", \"chr21_p\", \"chr22_p\", \"chrM_p\", \"chrY_p\", \"chrY_q\", \"chrX_p\", \"chrX_q\"]\n",
    "# hg38_arms = hg38_arms_full[~hg38_arms_full[\"name\"].isin(excluded_arms)].reset_index(drop=True)\n",
    "\n",
    "# can do 1 chromosome (or arm) as well ..\n",
    "included_arms = [\"chr1_q\", \"chr2_p\", \"chr4_q\", \"chr6_q\"]\n",
    "included_arms = hg38_arms_full[\"name\"].to_list()[:44] # all autosomal ones ...\n",
    "hg38_arms = hg38_arms_full[hg38_arms_full[\"name\"].isin(included_arms)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df76e50c-9801-4dee-b60e-9cb961cb4996",
   "metadata": {},
   "source": [
    "# There is a problem with our arms view of the chromosomes ...\n",
    "\n",
    "the way we do it now - end of p-arm is alsways equal to the start of q-arm ...\n",
    "\n",
    "After binning this could lead to the situation where last bin of p-arm is upstream of the first q-arm bin ...\n",
    "\n",
    "This makes `cooltools.api.is_valid_expected` crash ...\n",
    "\n",
    "Let's try solving that by adding 1 bp to the start of every q-arm ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514c091e-9efa-43fd-804b-bd902ce155c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_arm_view(\n",
    "    view_df,\n",
    "    binsize,\n",
    "):\n",
    "    \"\"\"\n",
    "    adjust arm-based view of the genome to fix slightly overlapping p and q arms ...\n",
    "    \"\"\"\n",
    "    _iter_view = view_df.itertuples(index=False)\n",
    "    return pd.DataFrame(\n",
    "        [(c,s+binsize,e,n) if (\"q\" in n) else (c,s,e,n) for c,s,e,n in _iter_view],\n",
    "        columns=hg38_arms.columns\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd03381-422b-45ed-a602-a3a79a5810bd",
   "metadata": {},
   "source": [
    "## Now let's get to pileups ! First - calcualte expected for all samples ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bf0338-66b5-440b-84a6-9eb9a15c898a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cooler files that we'll work on :\n",
    "binsize10 = 10_000\n",
    "telo_clrs10 = { _k: cooler.Cooler(f\"{_path}::/resolutions/{binsize10}\") for _k, _path in telo_dict.items() }\n",
    "\n",
    "# cooler files that we'll work on :\n",
    "binsize25 = 25_000\n",
    "telo_clrs25 = { _k: cooler.Cooler(f\"{_path}::/resolutions/{binsize25}\") for _k, _path in telo_dict.items() }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76e8980-80c8-48f0-b4dc-82d24e08c738",
   "metadata": {},
   "source": [
    "## We'll continue this dance between 10 and 25 kb resolutions - one for cis and one for trans ...\n",
    "\n",
    "### cis-expected first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6ad3e7-3b95-4d7f-9fae-dfbbe76b4e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _job(packed_data, sample):\n",
    "    # packed data -> exp_kwargs and a dict with coolers for each sample\n",
    "    exp_kwargs, clr_dict = packed_data\n",
    "    _clr = clr_dict[sample]\n",
    "    # in order to use spawn/forkserver we have to import for worker\n",
    "    from cooltools import expected_cis\n",
    "    _exp = expected_cis( _clr, **exp_kwargs)\n",
    "    return (sample, _exp)\n",
    "\n",
    "# define expected parameters in the form of kwargs-dict:\n",
    "exp_kwargs = dict(\n",
    "    view_df=adjust_arm_view(hg38_arms, binsize10),\n",
    "    intra_only=False,\n",
    "    nproc=12\n",
    ")\n",
    "\n",
    "# have to use daemon=False, because _job is multiprocessing-based already ...\n",
    "with WorkerPool(\n",
    "    n_jobs=8,\n",
    "    daemon=False,\n",
    "    shared_objects=( exp_kwargs, telo_clrs10 ),\n",
    "    start_method=\"forkserver\",  # little faster than spawn, fork is the fastest\n",
    "    use_dill=True,\n",
    ") as wpool:\n",
    "    results = wpool.map(_job, telo_clrs10, progress_bar=True)\n",
    "\n",
    "# sort out the results ...\n",
    "telo_exps_cis = {sample: _exp for sample, _exp in results}\n",
    "# # old way of doing it\n",
    "# telo_exps_cis = {k: cooltools.expected_cis( _clr, **exp_kwargs) for k, _clr in telo_clrs10.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e403b6-b7cd-4270-80cb-82eb81177e06",
   "metadata": {},
   "source": [
    "### trans-expected second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2eabf4-30f6-4c2a-a063-a66e833fb587",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _job(packed_data, sample):\n",
    "    # unpack data\n",
    "    clr_dict, = packed_data\n",
    "    exp_kwargs = dict(chunksize=1000000, nproc=12)\n",
    "    from cooltools import expected_trans\n",
    "    _clr = clr_dict[sample]\n",
    "    _exp = expected_trans( _clr, **exp_kwargs).set_index([\"region1\", \"region2\"]).sort_index()\n",
    "    return (sample, _exp)\n",
    "\n",
    "# have to use daemon=False, because _job is multiprocessing-based already ...\n",
    "with WorkerPool(\n",
    "    n_jobs=8,\n",
    "    daemon=False,\n",
    "    shared_objects=(telo_clrs25, ),\n",
    "    start_method=\"forkserver\",\n",
    "    use_dill=True,\n",
    ") as wpool:\n",
    "    results = wpool.map(_job, telo_clrs25, progress_bar=True)\n",
    "\n",
    "# sort out the results ...\n",
    "telo_exps_trans = {sample: _exp for sample, _exp in results}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d4ef7a-e9fd-4523-a312-ce7a4699c10c",
   "metadata": {},
   "source": [
    "# Read pre-called native compartments (anchors) and pick one for pileups:\n",
    "## Skip all of the anchor characterization and annotation ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25bfd74-7d0f-483b-9061-9d061e14c792",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_anchor_fnames = {\n",
    "    \"mega_2X_enrichment\": \"ID_anchors/mega_2X_enrichment.fourth_mega.max_size.bed\",\n",
    "    \"5hr_2X_enrichment_old\": \"ID_anchors/5hr_2X_enrichment.second_bulk.max_size.bed\",\n",
    "    \"5hr_2X_enrichment\": \"ID_anchors/5hr_2X_enrichment.pixel_derived.bed\",\n",
    "    \"5hr_2X_enrichment_nosing\": \"ID_anchors/5hr_2X_enrichment.pixel_derived.no_singletons.bed\",\n",
    "    \"5hr_notinCyto_2X_enrichment_signal\": \"ID_anchors/p5notin_pCyto_anchors_2X_enrichment.pixel_derived.signal_peaks.bed\",\n",
    "    \"5hr_2X_enrichment_signal\": \"ID_anchors/5hr_2X_enrichment.pixel_derived.signal_peaks.bed\",\n",
    "    \"10hr_2X_enrichment_signal\": \"ID_anchors/10hrs_2X_enrichment.pixel_derived.signal_peaks.bed\",\n",
    "    \"N93p5_2X_enrichment_signal\": \"ID_anchors/N93p5_2X_enrichment.pixel_derived.signal_peaks.bed\",\n",
    "    \"pCyto_2X_enrichment_signal\": \"ID_anchors/pCyto_2X_enrichment.pixel_derived.signal_peaks.bed\",\n",
    "    \"mCyto_2X_enrichment_signal\": \"ID_anchors/mCyto_2X_enrichment.pixel_derived.signal_peaks.bed\",\n",
    "    \"mega_3X_enrichment\": \"ID_anchors/mega_3X_enrichment.fifth_mega3x.max_size.bed\",\n",
    "    \"MEGA_2X_enrichment\": \"ID_anchors/MEGAp5_2X_enrichment.pixel_derived.signal_peaks.bed\",\n",
    "    \"MEGA_weaker_2X_enrichment\": \"ID_anchors/MEGA_plus_weak_anchors_2X_enrichment.pixel_derived.signal_peaks.bed\",\n",
    "    \"MEGAN93_2X_enrichment\": \"ID_anchors/MEGAN93p5_2X_enrichment.pixel_derived.signal_peaks.bed\",\n",
    "    \"MEGAminus_2X_enrichment\": \"ID_anchors/MEGA_minus_ctrl_2X_enrichment.pixel_derived.signal_peaks.bed\",\n",
    "    \"cyto_2x_enrichment\": \"ID_anchors/cyto_2x_enrichment.third_mCyto.max_size.bed\",\n",
    "}\n",
    "\n",
    "id_anchors_dict = {}\n",
    "for id_name, fname in id_anchor_fnames.items():\n",
    "    id_anchors_dict[id_name] = pd.read_csv(fname, sep=\"\\t\")\n",
    "    # ...\n",
    "    print(f\"loaded {len(id_anchors_dict[id_name]):5d} ID anchors {id_name:>20} in BED format ...\")\n",
    "\n",
    "\n",
    "_anchors = id_anchors_dict[\"pCyto_2X_enrichment_signal\"]\n",
    "_anchors_fname = id_anchor_fnames[\"pCyto_2X_enrichment_signal\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab970ca-1fc7-48d2-b607-0206ab73eb83",
   "metadata": {},
   "source": [
    "# Create combinations of `_anchors` for the pileup - i.e. the all-by-all type of situation !\n",
    "## cis and trans - done a bit differently ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af90fdd-ac68-43cb-9ce3-0874f7f5610d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cis ...\n",
    "_df = bioframe.pair_by_distance(\n",
    "    _anchors,\n",
    "    min_sep=0,\n",
    "    max_sep=100_000_000,\n",
    "    relative_to='midpoints',\n",
    "    keep_order=True,\n",
    "    suffixes=('1', '2'),\n",
    ")\n",
    "\n",
    "# calculate the distance from the diagonal ...\n",
    "_df[\"dist\"] = _df.eval(\".5*(start2+end2) - .5*(start1+end1)\")\n",
    "\n",
    "# we have to explicitly keep only intra-arm interactions for this to work !!!!!!!!\n",
    "_intra_arm_index = cooltools.lib.common.assign_view_auto(_df, adjust_arm_view(hg38_arms, binsize10)).query(\"region1 == region2\").index\n",
    "_df_intra_arm = _df.loc[_intra_arm_index].reset_index(drop=True)\n",
    "display(_df_intra_arm)\n",
    "\n",
    "# trans ...\n",
    "# use that recipy from stackoverflow using `triu_indices` ...\n",
    "# try all pairwise combinations on a smaller subset ...\n",
    "_left, _right = np.triu_indices(len(_anchors), k=1)\n",
    "\n",
    "ALL_pairwise_anchors = pd.concat(\n",
    "    [\n",
    "        _anchors.iloc[_left].add_suffix(\"1\").reset_index(drop=True),\n",
    "        _anchors.iloc[_right].add_suffix(\"2\").reset_index(drop=True)\n",
    "    ],\n",
    "    #\n",
    "    axis=1\n",
    " )\n",
    "tr_feat = ALL_pairwise_anchors.query(\"chrom1 != chrom2\").reset_index(drop=True)\n",
    "display(tr_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb665819-b5c4-46ca-ae92-2ad216b3c1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2545bb-f3e1-4c78-bbcd-522a1335aadf",
   "metadata": {},
   "source": [
    "# This is the step of recaluclating trans-pileups\n",
    "\n",
    "keep intermediate results in `/data/dekkerlab/tmp`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536a49d4-10c8-4527-adc9-7e873e533df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "recalculate_trans_oe_pups = True\n",
    "# # check if intermediate results are still there ...\n",
    "# if not all(os.path.isfile(f\"/data/dekkerlab/tmp/new_{s}_trans_stack_oe.npy\") for s in telo_clrs25):\n",
    "#     print(\"some of the tmp npy files are missing ! need to recalculate! \")\n",
    "#     recalculate_trans_oe_pups = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086e6e95-3a3f-4261-9de5-33ccd78d4073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! ls -lah /data/sergpolly/tmp/*.npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a30d4b-73de-48dd-9d62-809aa24c8b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this takes up a LOT of RAM ...\n",
    "if recalculate_trans_oe_pups:\n",
    "    # ...\n",
    "    def _job(packed_data, sample):\n",
    "        # unpack data\n",
    "        clr_dict, features, exp_dict = packed_data\n",
    "        kwargs = dict(flank=100_000, min_diag=None, nproc=18)\n",
    "        from cooltools import pileup\n",
    "        _clr = clr_dict[sample]\n",
    "        _stack = pileup( _clr, features, **kwargs)\n",
    "        # now divide that stack by expected:\n",
    "        _exp = exp_dict[sample]\n",
    "        features_chroms_order = features.set_index([\"chrom1\",\"chrom2\"]).index\n",
    "        _stack_exp = _exp.loc[features_chroms_order, \"balanced.avg\"].to_numpy()\n",
    "        import numpy as np\n",
    "        np.save(f\"/data/sergpolly/tmp/new_{sample}_trans_stack_oe.npy\", _stack/_stack_exp[:,None,None])\n",
    "        return True\n",
    "\n",
    "    # have to use daemon=False, because _job is multiprocessing-based already ...\n",
    "    with WorkerPool(\n",
    "        n_jobs=4,\n",
    "        daemon=False,\n",
    "        shared_objects=(telo_clrs25, tr_feat, telo_exps_trans),\n",
    "        start_method=\"forkserver\",\n",
    "        use_dill=True,\n",
    "    ) as wpool:\n",
    "        results = wpool.map(_job, telo_clrs25, progress_bar=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273ea115-5364-4149-bf5c-68c8231bfc92",
   "metadata": {},
   "source": [
    "## Do cis pielups in parallel ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5562e014-90ea-4ef8-b93b-26158ca6fe6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _job(packed_data, sample):\n",
    "    # unpack shared data\n",
    "    features, clr_dict, exp_dict, view_df = packed_data\n",
    "    pup_kwargs = dict(view_df=view_df, flank=100_000, nproc=12)\n",
    "    _clr = clr_dict[sample]\n",
    "    _exp = exp_dict[sample]\n",
    "    from cooltools import pileup\n",
    "    _pstack = pileup(\n",
    "        _clr,\n",
    "        features,\n",
    "        expected_df=_exp,\n",
    "        **pup_kwargs,\n",
    "    )\n",
    "    return (sample, _pstack)\n",
    "\n",
    "# have to use daemon=False, because _job is multiprocessing-based already ...\n",
    "with WorkerPool(\n",
    "    n_jobs=8,\n",
    "    daemon=False,\n",
    "    shared_objects=(_df_intra_arm, telo_clrs10, telo_exps_cis, adjust_arm_view(hg38_arms, binsize10)),\n",
    "    start_method=\"forkserver\",\n",
    "    use_dill=True,\n",
    ") as wpool:\n",
    "    results = wpool.map(_job, telo_clrs10, progress_bar=True)\n",
    "\n",
    "# sort out the results ...\n",
    "fullstacks_cis = {sample: _pstack for sample, _pstack in results}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46c62fc-b62c-4878-b853-a4f60086ff2c",
   "metadata": {},
   "source": [
    "## Now let's store all of the results in a single HDF5 file for convenience ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91e48db-b04e-416d-a600-d048dcad50e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09f50eb-ec3b-48b4-8ce5-a8ea9befc5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(\"/data/sergpolly/tmp/Pileups_ID_by_distance_pCyto.hdf5\", 'x') as f:\n",
    "    # add metadata just in case\n",
    "    f.attrs[\"cis_binsize\"] = binsize10\n",
    "    f.attrs[\"trans_binsize\"] = binsize25\n",
    "    f.attrs[\"id_anchors_fname\"] = _anchors_fname\n",
    "    # CIS ...\n",
    "    print(\"saving cis data ...\")\n",
    "    _cis_grp = f.create_group(\"cis\")\n",
    "    # a group for indices - i.e. _anchors pairwise\n",
    "    _idxs_subgrp = _cis_grp.create_group(\"indices\")\n",
    "    _idxs_subgrp.create_dataset(\"anchor1\", data=_df_intra_arm[\"cluster1\"].to_numpy())\n",
    "    _idxs_subgrp.create_dataset(\"anchor2\", data=_df_intra_arm[\"cluster2\"].to_numpy())\n",
    "    # a group for pileups - i.e. 3D array\n",
    "    _pups_subgrp = _cis_grp.create_group(\"pileups\")\n",
    "    # create cis pileups dataset ...\n",
    "    for _sample, _arr in fullstacks_cis.items():\n",
    "        _pups_subgrp.create_dataset(_sample, data=_arr)\n",
    "    #\n",
    "    #\n",
    "    # TRANS ...\n",
    "    print(\"re-saving trans data ...\")\n",
    "    _trans_grp = f.create_group(\"trans\")\n",
    "    # a group for indices - i.e. _anchors pairwise\n",
    "    _idxs_subgrp = _trans_grp.create_group(\"indices\")\n",
    "    _idxs_subgrp.create_dataset(\"anchor1\", data=tr_feat[\"cluster1\"].to_numpy())\n",
    "    _idxs_subgrp.create_dataset(\"anchor2\", data=tr_feat[\"cluster2\"].to_numpy())\n",
    "    # a group for pileups - i.e. 3D array\n",
    "    _pups_subgrp = _trans_grp.create_group(\"pileups\")\n",
    "    # create trans pileups dataset ...\n",
    "    for _sample in telo_clrs10:\n",
    "        print(f\"    {_sample} ...\")\n",
    "        _arr = np.load(f\"/data/sergpolly/tmp/new_{_sample}_trans_stack_oe.npy\")\n",
    "        _pups_subgrp.create_dataset(_sample, data=_arr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc178436-4fb9-466c-ac54-3b95644678ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls -lah /data/sergpolly/tmp/Pileups_ID_by_distance_pCyto.hdf5\n",
    "# ! rm /data/sergpolly/tmp/Pileups_ID_by_distance_pCyto.hdf5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbade5fa-dc8e-45f8-917b-c21b996b1d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls -lah /data/sergpolly/tmp/Pileups_ID_by_distance.hdf5\n",
    "# ! rm /data/sergpolly/tmp/Pileups_ID_by_distance.hdf5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490cf096-bd28-48cf-9189-2bfb010e5582",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ce87a1-4318-4342-a864-fcd9498e1e2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2401dbd5-78e5-41a6-ba0b-c5cc59816f24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f196a217-c23b-4853-a98b-16ba9cb35367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create indexes for pileup groups\n",
    "# _enrich1_idx = tr_feat.query(\"(dots1>0)&(dots2>0)\").index\n",
    "# _enrich10_idx = tr_feat.query(\"(dots_footprint1==0)&(dots_footprint2==0)\").index\n",
    "# _deplete_idx = tr_feat.query(\"(dots_footprint1>0)&(dots_footprint2>0)\").index\n",
    "# # _enrich1_idx = tr_feat.query(\"(valency1>1)&(valency2>1)\").index\n",
    "# # _enrich10_idx = tr_feat.query(\"(valency1>=10)&(valency2>=10)\").index\n",
    "# # _deplete_idx = tr_feat.query(\"(valency1==1)&(valency2==1)\").index\n",
    "# len(tr_feat), len(_enrich10_idx), len(_enrich1_idx), len(_deplete_idx)\n",
    "\n",
    "# # now average those sub-pileups :\n",
    "\n",
    "# def _job(sample):\n",
    "#     # extract stack of observed and expected per sample ...\n",
    "#     _stack = np.load(f\"/data/sergpolly/tmp/new_{sample}_trans_stack_oe.npy\")\n",
    "#     return (\n",
    "#         sample,\n",
    "#         np.nanmean(_stack[_enrich10_idx], axis=0),\n",
    "#         np.nanmean(_stack[_enrich1_idx], axis=0),\n",
    "#         np.nanmean(_stack[_deplete_idx], axis=0),\n",
    "#         np.nanmean(_stack, axis=0)\n",
    "#     )\n",
    "\n",
    "# with WorkerPool( n_jobs=4, start_method=\"fork\", use_dill=True ) as wpool:\n",
    "#     results = wpool.map(_job, telo_clrs25, progress_bar=True)\n",
    "# # unpack results ...\n",
    "# stack_means = {s: [e10, e1, ed, eall] for s, e10, e1, ed, eall in results }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a92d8d-f5ab-476d-ac55-a34df8b2403f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bdb68a-d901-4f07-b00b-68a5ab289a6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72129c2-7808-4700-a9b9-9048fba18aca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d16fe5-d8a9-43d2-9316-cfe20f0c7e52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7320aa5-7622-4341-a58b-130e6bd322e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ed07a4e2-560a-4a32-930d-53a35625da74",
   "metadata": {},
   "source": [
    "# plotting pups ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b559b6-bdf1-493a-86f7-32c0246f6756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pileup select samples only !\n",
    "_select_sample_groups = [\n",
    "    [\n",
    "        \"mMito\",\n",
    "        \"mTelo\",\n",
    "        \"mCyto\",\n",
    "        \"m5hR1R2\",\n",
    "        \"m10hR1R2\"\n",
    "    ],\n",
    "    # # p-ones\n",
    "    [\n",
    "        \"pMito\",\n",
    "        \"pTelo\",\n",
    "        \"pCyto\",\n",
    "        \"p5hR1R2\",\n",
    "        \"p10hR1R2\",\n",
    "    ],\n",
    "    # # # # the mix one - mp\n",
    "    [\n",
    "        \"N93m5\",\n",
    "        \"N93m10\",\n",
    "    ],\n",
    "    # p ...\n",
    "    [\n",
    "        \"N93p5\",\n",
    "        \"N93p10\",\n",
    "    ],\n",
    "    [\n",
    "        \"m10hR1R2\",\n",
    "        \"p10hR1R2\",\n",
    "        \"mp10hR1R2\",\n",
    "    ],\n",
    "    [\n",
    "        \"N93m10\",\n",
    "        \"N93p10\",\n",
    "        \"N93mp10\",\n",
    "    ],\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558ecf89-6650-4e06-891f-d3a9495556de",
   "metadata": {},
   "outputs": [],
   "source": [
    "_flank=100_000\n",
    "num_trans_groups = 4\n",
    "ggg = [ \"e10\", \"e1\", \"ed\", \"eall\" ]\n",
    "\n",
    "for _sample_group in _select_sample_groups:\n",
    "\n",
    "    f, axs = plt.subplots(\n",
    "        nrows=len(_sample_group),\n",
    "        ncols=len(ggg)+1,\n",
    "        figsize=(3*len(ggg), 3*len(_sample_group)),\n",
    "        width_ratios=[1]*len(ggg)+[0.05],\n",
    "        sharex=True,\n",
    "        sharey=True,\n",
    "    )\n",
    "\n",
    "    gs = axs[0, -1].get_gridspec()\n",
    "    # remove axes for the last column ...\n",
    "    for ax in axs[:, -1]:\n",
    "        ax.remove()\n",
    "\n",
    "    axcb = f.add_subplot(gs[1:3, -1])\n",
    "\n",
    "    for i, (_axs, k) in enumerate(zip(axs,_sample_group)):\n",
    "        # going over samples ...\n",
    "        _stacks = stack_means[k]\n",
    "        print(k)\n",
    "        for j, (ax, _q) in enumerate(zip(_axs, ggg)):\n",
    "            # going over groupings (by dist, or whatever ...)\n",
    "            _ccc = ax.imshow(\n",
    "                _stacks[j],\n",
    "                cmap='RdBu_r',\n",
    "                norm=LogNorm(vmin=1/2.5,vmax=2.5),\n",
    "                # norm=colors.CenteredNorm(vcenter=1,halfrange=0.9,clip=False),\n",
    "                # norm=colors.TwoSlopeNorm(1, vmin=0.5, vmax=2),\n",
    "                aspect=\"auto\",\n",
    "            )\n",
    "            _ccc.cmap.set_over(\"#400000\")\n",
    "            ticks_pixels = np.linspace(0, _flank*2//binsize25, 5)\n",
    "            ticks_kbp = ((ticks_pixels-ticks_pixels[-1]/2)*binsize25//1000).astype(int)\n",
    "            # ax.set_title(f\"{int(_q.left/1_000)} - {int(_q.right/1_000)} kp: {len(_mtx)}\")\n",
    "            if i == 0:\n",
    "                # top row\n",
    "                _axname = _q\n",
    "                ax.set_title(_axname)\n",
    "            if i == len(_sample_group)-1:\n",
    "                ax.set_xticks(ticks_pixels, ticks_kbp)\n",
    "                ax.set_xlabel('relative position, kbp')\n",
    "            ax.set_yticks(ticks_pixels, ticks_kbp)\n",
    "            if j<1:\n",
    "                ax.set_ylabel(f\"{k}\", fontsize=14)\n",
    "\n",
    "    plt.colorbar(_ccc, label=\"obs/exp\", cax=axcb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf45b2b-4152-4259-a83e-20b7b9a2ff5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "_flank = 100_000\n",
    "# _dfff = arm_feat.query(\"(valency1>1)&(valency2>1)\")\n",
    "_dfff = arm_feat\n",
    "#_dfff = arm_feat.query(\"((H3K27ac1>4)&(H3K27ac2>4))&(valency1>2)&(valency2>2)\")\n",
    "_dfff = arm_feat.query(\"(dots1>0)&(dots2>0)\")\n",
    "\n",
    "print(f\"dealing with {len(_dfff)} elements total ...\")\n",
    "dist_bins = [0, 2_500_000, 50_000_000, 90_000_000, 170_000_000, 1_000_000_000]\n",
    "ggg = _dfff.groupby(pd.cut( _dfff[\"dist\"], dist_bins ))\n",
    "nquants = len(ggg)\n",
    "\n",
    "for _sample_group in _select_sample_groups:\n",
    "\n",
    "    f, axs = plt.subplots(\n",
    "        nrows=len(_sample_group),\n",
    "        ncols=len(ggg)+1,\n",
    "        figsize=(3*len(ggg), 3*len(_sample_group)),\n",
    "        width_ratios=[1]*nquants+[0.05],\n",
    "        sharex=True,\n",
    "        sharey=True,\n",
    "    )\n",
    "\n",
    "    gs = axs[0, -1].get_gridspec()\n",
    "    # remove axes for the last column ...\n",
    "    for ax in axs[:, -1]:\n",
    "        ax.remove()\n",
    "\n",
    "    axcb = f.add_subplot(gs[1:3, -1])\n",
    "\n",
    "    for i, (_axs, k) in enumerate(zip(axs,_sample_group)):\n",
    "        # going over samples ...\n",
    "        _stacks = fullstacks_arm_cis[k]\n",
    "        print(k)\n",
    "        for j, (ax, (_q, _mtx)) in enumerate(zip(_axs, ggg.groups.items())):\n",
    "            # going over groupings (by dist, or whatever ...)\n",
    "            _ccc = ax.imshow(\n",
    "                np.nanmean(_stacks[_mtx], axis=0),\n",
    "                cmap='RdBu_r',\n",
    "                norm=LogNorm(vmin=1/2.5,vmax=2.5),\n",
    "                # norm=colors.CenteredNorm(vcenter=1,halfrange=0.9,clip=False),\n",
    "                # norm=colors.TwoSlopeNorm(1, vmin=0.5, vmax=2),\n",
    "                aspect=\"auto\",\n",
    "            )\n",
    "            _ccc.cmap.set_over(\"#400000\")\n",
    "            ticks_pixels = np.linspace(0, _flank*2//binsize10, 5)\n",
    "            ticks_kbp = ((ticks_pixels-ticks_pixels[-1]/2)*binsize10//1000).astype(int)\n",
    "            # ax.set_title(f\"{int(_q.left/1_000)} - {int(_q.right/1_000)} kp: {len(_mtx)}\")\n",
    "            if i == 0:\n",
    "                # top row\n",
    "                _axname = _get_name(_q.left, _q.right, len(_mtx))\n",
    "                ax.set_title(_axname)\n",
    "            if i == len(_sample_group)-1:\n",
    "                ax.set_xticks(ticks_pixels, ticks_kbp)\n",
    "                ax.set_xlabel('relative position, kbp')\n",
    "            ax.set_yticks(ticks_pixels, ticks_kbp)\n",
    "            if j<1:\n",
    "                ax.set_ylabel(f\"{k}\", fontsize=14)\n",
    "\n",
    "    plt.colorbar(_ccc, label=\"obs/exp\", cax=axcb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c901e9b-cfe8-4ff1-a624-c288a71e4698",
   "metadata": {},
   "outputs": [],
   "source": [
    "_df_intra_arm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d5608e-c9a2-4b1e-b228-5a8f1e278614",
   "metadata": {},
   "outputs": [],
   "source": [
    "1400+700"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de606d99-7137-442b-914d-459257926ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "_anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee61962d-80db-49cd-a37a-9ec243b0384a",
   "metadata": {},
   "outputs": [],
   "source": [
    "_flank = 100_000\n",
    "_dfff = _df_intra_arm\n",
    "# _dfff = _df_intra_arm.query(\"(valency1>1)&(valency2>1)\")\n",
    "# _dfff = _df_intra_arm.query(\"(ctcf1<1.9)&(ctcf2<1.9)\")\n",
    "# _dfff = _df_intra_arm.query(\"(dots1>0)&(dots2>0)\")\n",
    "# _dfff = _df_intra_arm.query(\"(dots_footprint1==0)&(dots_footprint2==0)\")\n",
    "#_dfff = _df_intra_arm.query(\"((H3K27ac1>4)&(H3K27ac2>4))&(valency1>2)&(valency2>2)\")\n",
    "\n",
    "print(f\"dealing with {len(_dfff)} elements total ...\")\n",
    "# dist_bins = [0, 300_000, 1_000_000, 10_000_000, 1_000_000_000]\n",
    "dist_bins = [0, 250_000, 500_000, 1_000_000, 2_500_000, 5_000_000, 10_000_000, 1_000_000_000]\n",
    "ggg = _dfff.groupby(pd.cut( _dfff[\"dist\"], dist_bins ), observed=True)\n",
    "nquants = len(ggg)\n",
    "\n",
    "for _sample_group in _select_sample_groups:\n",
    "\n",
    "    f, axs = plt.subplots(\n",
    "        nrows=len(_sample_group),\n",
    "        ncols=len(ggg)+1,\n",
    "        figsize=(3*len(ggg), 3*len(_sample_group)),\n",
    "        width_ratios=[1]*nquants+[0.05],\n",
    "        sharex=True,\n",
    "        sharey=True,\n",
    "    )\n",
    "\n",
    "    gs = axs[0, -1].get_gridspec()\n",
    "    # remove axes for the last column ...\n",
    "    for ax in axs[:, -1]:\n",
    "        ax.remove()\n",
    "\n",
    "    axcb = f.add_subplot(gs[1:3, -1])\n",
    "\n",
    "    for i, (_axs, k) in enumerate(zip(axs,_sample_group)):\n",
    "        # going over samples ...\n",
    "        _stacks = fullstacks_cis[k]\n",
    "        print(k)\n",
    "        for j, (ax, (_q, _mtx)) in enumerate(zip(_axs, ggg.groups.items())):\n",
    "            # going over groupings (by dist, or whatever ...)\n",
    "            _ccc = ax.imshow(\n",
    "                np.nanmean(_stacks[_mtx], axis=0),\n",
    "                cmap='RdBu_r',\n",
    "                norm=LogNorm(vmin=1/2.5,vmax=2.5),\n",
    "                # norm=colors.CenteredNorm(vcenter=1,halfrange=0.9,clip=False),\n",
    "                # norm=colors.TwoSlopeNorm(1, vmin=0.5, vmax=2),\n",
    "                aspect=\"auto\",\n",
    "            )\n",
    "            _ccc.cmap.set_over(\"#400000\")\n",
    "            ticks_pixels = np.linspace(0, _flank*2//binsize10, 5)\n",
    "            ticks_kbp = ((ticks_pixels-ticks_pixels[-1]/2)*binsize10//1000).astype(int)\n",
    "            # ax.set_title(f\"{int(_q.left/1_000)} - {int(_q.right/1_000)} kp: {len(_mtx)}\")\n",
    "            if i == 0:\n",
    "                # top row\n",
    "                _axname = _get_name(_q.left, _q.right, len(_mtx))\n",
    "                ax.set_title(_axname)\n",
    "            if i == len(_sample_group)-1:\n",
    "                ax.set_xticks(ticks_pixels, ticks_kbp)\n",
    "                ax.set_xlabel('relative position, kbp')\n",
    "            ax.set_yticks(ticks_pixels, ticks_kbp)\n",
    "            if j<1:\n",
    "                ax.set_ylabel(f\"{k}\", fontsize=14)\n",
    "\n",
    "    plt.colorbar(_ccc, label=\"obs/exp\", cax=axcb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c4800e-fe77-4bf1-acef-c04f73cba206",
   "metadata": {},
   "outputs": [],
   "source": [
    "_df_intra_arm[\"H3K27ac1\"].hist(bins=np.linspace(0,120, 100) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a6f6a8-a2c7-4178-b042-ee8404b0c801",
   "metadata": {},
   "outputs": [],
   "source": [
    "_anchors[\"H3K27ac\"].hist(bins=np.linspace(0,120, 100) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de61bad2-c176-4a63-8260-2f760b8093a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "_flank = 100_000\n",
    "_dfff = _df_intra_arm\n",
    "# _dfff = _df_intra_arm.query(\"(valency1>1)&(valency2>1)\")\n",
    "# _dfff = _df_intra_arm.query(\"(size1>20_000)&(size2>20_000)\")\n",
    "# _dfff = _df_intra_arm.query(\"(size2>70_000)\")\n",
    "\n",
    "#_dfff = _df_intra_arm.query(\"((H3K27ac1>4)&(H3K27ac2>4))&(valency1>2)&(valency2>2)\")\n",
    "_dfff = _df_intra_arm.query(\"(H3K27ac1<1)&(H3K27ac2<1)\")\n",
    "\n",
    "print(f\"dealing with {len(_dfff)} elements total ...\")\n",
    "# dist_bins = [0, 300_000, 1_000_000, 1_500_000, 3_000_000, 5_000_000, 10_000_000, 1_000_000_000]\n",
    "dist_bins = [0, 500_000, 10_000_000, 1_000_000_000]\n",
    "# dist_bins = [0, 300_000, 1_000_000, 1_500_000, 3_000_000, 5_000_000, 10_000_000, 1_000_000_000]\n",
    "# dist_bins = [0, 1_000_000_000]\n",
    "ggg = _dfff.groupby(pd.cut( _dfff[\"dist\"], dist_bins ), observed=True)\n",
    "\n",
    "trans_category = \"eall\"\n",
    "nquants = len(ggg)\n",
    "\n",
    "for _sample_group in _select_sample_groups:\n",
    "\n",
    "    f, axs = plt.subplots(\n",
    "        nrows=len(_sample_group),\n",
    "        ncols=len(ggg)+1+1,\n",
    "        # ncols=len(ggg),\n",
    "        figsize=(3*(len(ggg)+1), 3*len(_sample_group)),\n",
    "        width_ratios=[1]*(len(ggg)+1)+[0.05],\n",
    "        sharex=False,\n",
    "        sharey=False,\n",
    "    )\n",
    "\n",
    "    gs = axs[0, -1].get_gridspec()\n",
    "    # remove axes for the last column ...\n",
    "    for ax in axs[:, -1]:\n",
    "        ax.remove()\n",
    "\n",
    "    axcb = f.add_subplot(gs[1:3, -1])\n",
    "\n",
    "    # imshow ...\n",
    "    imshow_kwargs = dict(\n",
    "        cmap='RdBu_r',\n",
    "        norm=LogNorm(vmin=1/2.5,vmax=2.5),\n",
    "        aspect=1,\n",
    "    )\n",
    "\n",
    "    for i, (_axs, k) in enumerate(zip(axs,_sample_group)):\n",
    "        # going over samples ...\n",
    "        _stacks = fullstacks_cis[k]\n",
    "        print(k)\n",
    "        for j, (ax, (_q, _mtx)) in enumerate(zip(_axs, ggg.groups.items())):\n",
    "            # going over groupings (by dist, or whatever ...)\n",
    "            _ccc = ax.imshow( np.nanmean(_stacks[_mtx], axis=0), **imshow_kwargs )\n",
    "            _ccc.cmap.set_over(\"#300000\")\n",
    "            # _ccc.cmap.set_over(\"black\")\n",
    "            ticks_pixels = np.linspace(0, _flank*2//binsize10, 5)\n",
    "            ticks_kbp = ((ticks_pixels-ticks_pixels[-1]/2)*binsize10//1000).astype(int)\n",
    "            if i == 0:\n",
    "                # top row\n",
    "                _axname = _get_name(_q.left, _q.right, len(_mtx))\n",
    "                ax.set_title(_axname)\n",
    "            if i == len(_sample_group)-1:\n",
    "                ax.set_xticks(ticks_pixels, ticks_kbp)\n",
    "                ax.set_xlabel('relative position, kbp')\n",
    "            else:\n",
    "                ax.set_xticks(ticks_pixels,[])\n",
    "            if j<1:\n",
    "                ax.set_ylabel(f\"{k}\", fontsize=14)\n",
    "                ax.set_yticks(ticks_pixels, ticks_kbp)\n",
    "            else:\n",
    "                ax.set_yticks(ticks_pixels,[])\n",
    "        # plot trans separately after all ...\n",
    "        # going over groupings (by dist, or whatever ...)\n",
    "        j = j + 1\n",
    "        ax = _axs[j]\n",
    "        _stacks = stack_means[k][1]  # the e1 one\n",
    "        # we need to adjust from 250_000 flank to 100_000 one ...\n",
    "        _ccc = ax.imshow( _stacks, **imshow_kwargs )\n",
    "        _ccc.cmap.set_over(\"#300000\")\n",
    "        # _ccc.cmap.set_over(\"black\")\n",
    "        ticks_pixels = np.linspace(0, _flank*2//binsize25, 5)\n",
    "        ticks_kbp = ((ticks_pixels-ticks_pixels[-1]/2)*binsize25//1000).astype(int)\n",
    "        if i == 0:\n",
    "            # top row\n",
    "            _axname = \"trans\"\n",
    "            ax.set_title(_axname)\n",
    "        if i == len(_sample_group)-1:\n",
    "            ax.set_xticks(ticks_pixels, ticks_kbp)\n",
    "            ax.set_xlabel('relative position, kbp')\n",
    "        else:\n",
    "            ax.set_xticks(ticks_pixels, [])\n",
    "        if j<1:\n",
    "            ax.set_ylabel(f\"{trans_category}\", fontsize=14)\n",
    "            ax.set_yticks(ticks_pixels, ticks_kbp)\n",
    "        else:\n",
    "            ax.set_yticks(ticks_pixels,[])\n",
    "    plt.colorbar(_ccc, label=\"obs/exp\", cax=axcb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe119bc7-ce5f-45a8-8248-e8eb1800406b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c1d529-5a71-41dc-a025-be928f11576b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
