{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100a1b4f-b85a-48d3-9374-77358d8fd8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the number of threads for many common libraries\n",
    "from os import environ\n",
    "N_THREADS = '1'\n",
    "environ['OMP_NUM_THREADS'] = N_THREADS\n",
    "environ['OPENBLAS_NUM_THREADS'] = N_THREADS\n",
    "environ['MKL_NUM_THREADS'] = N_THREADS\n",
    "environ['VECLIB_MAXIMUM_THREADS'] = N_THREADS\n",
    "environ['NUMEXPR_NUM_THREADS'] = N_THREADS\n",
    "# https://superfastpython.com/numpy-number-blas-threads/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3e7973",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import chain\n",
    "\n",
    "# Hi-C utilities imports:\n",
    "import cooler\n",
    "import bioframe\n",
    "import cooltools\n",
    "from cooltools.lib.numutils import fill_diag\n",
    "\n",
    "# Visualization imports:\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib.ticker import EngFormatter\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "from itertools import cycle\n",
    "\n",
    "# from ipywidgets import interact, fixed\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4720d88-b737-4afb-b986-fb21cc569449",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cooltools.lib.plotting\n",
    "# just to get to the fall colormap ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d743c5",
   "metadata": {},
   "source": [
    "### Import modified \"guts\" of the dotfinder submodule from `helper_func` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1c6ae5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from helper_func import (\n",
    "    draw_kernel,\n",
    "    get_adjusted_expected_tile_OE,\n",
    "    score_tile_custom_cols_OE,\n",
    "    score_pixels_only_OE,\n",
    "    get_adjusted_expected_tile_more_columns,\n",
    "    score_tile_custom_cols,\n",
    "    score_pixels_only,\n",
    "    rectangles_around_dots,\n",
    ")\n",
    "score_tile_custom_cols_OE\n",
    "# turns out still need some of the dotfinder guts in here\n",
    "from cooltools.api.dotfinder import bp_to_bins, generate_tiles_diag_band\n",
    "from cooltools.lib.numutils import LazyToeplitz\n",
    "from cooltools.lib.common import assign_regions\n",
    "\n",
    "# from datashader.mpl_ext import dsshow, alpha_colormap\n",
    "# import datashader as ds\n",
    "# import datashader.transfer_functions as tf\n",
    "from functools import partial\n",
    "from data_catalog import bws, bws_vlim, telo_dict\n",
    "\n",
    "# optics ...\n",
    "from sklearn.cluster import OPTICS\n",
    "# find peaks to define anchors ...\n",
    "from scipy.signal import find_peaks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6727d9cd",
   "metadata": {},
   "source": [
    "### pick a dataset and binsize to work on ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b49590",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # ! pip install --upgrade --no-cache --no-deps --ignore-install cooler\n",
    "# # ls /home/dekkerlab/dots-test\n",
    "# # import higlass as hg\n",
    "# import jscatter\n",
    "import scipy\n",
    "import logging\n",
    "import multiprocess as mp\n",
    "# import mpire for nested multi-processing\n",
    "from mpire import WorkerPool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474819ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 10 kb is a resolution at which one can clearly see \"dots\":\n",
    "binsize = 10_000\n",
    "# cooler files that we'll work on :\n",
    "telo_clrs = { _k: cooler.Cooler(f\"{_path}::/resolutions/{binsize}\") for _k, _path in telo_dict.items() }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cb8d1b",
   "metadata": {},
   "source": [
    "### pick a region to work on ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372737ae-d867-427f-8517-6cd7719b7359",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use bioframe to fetch the genomic features from the UCSC.\n",
    "hg38_chromsizes = bioframe.fetch_chromsizes('hg38')\n",
    "hg38_cens = bioframe.fetch_centromeres('hg38')\n",
    "hg38_arms_full = bioframe.make_chromarms(hg38_chromsizes, hg38_cens)\n",
    "# only autosomal chromosomes ...\n",
    "included_arms = hg38_arms_full[\"name\"].to_list()[:44]\n",
    "hg38_arms = hg38_arms_full[hg38_arms_full[\"name\"].isin(included_arms)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14ab6eb",
   "metadata": {},
   "source": [
    "### pre-calculate expected for the cooler ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fe2528",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _job(packed_data, sample):\n",
    "    # packed data -> exp_kwargs and a dict with coolers for each sample\n",
    "    exp_kwargs, clr_dict = packed_data\n",
    "    _clr = clr_dict[sample]\n",
    "    # in order to use spawn/forkserver we have to import for worker\n",
    "    from cooltools import expected_cis\n",
    "    _exp = expected_cis( _clr, **exp_kwargs)\n",
    "    return (sample, _exp)\n",
    "\n",
    "# define expected parameters in the form of kwargs-dict:\n",
    "exp_kwargs = dict(\n",
    "    view_df=hg38_arms,\n",
    "    intra_only=False,\n",
    "    nproc=12\n",
    ")\n",
    "\n",
    "# have to use daemon=False, because _job is multiprocessing-based already ...\n",
    "with WorkerPool(\n",
    "    n_jobs=8,\n",
    "    daemon=False,\n",
    "    shared_objects=( exp_kwargs, telo_clrs ),\n",
    "    start_method=\"spawn\",  # little faster than spawn, fork is the fastest\n",
    "    use_dill=True,\n",
    ") as wpool:\n",
    "    results = wpool.map(_job, telo_clrs, progress_bar=True)\n",
    "\n",
    "# sort out the results ...\n",
    "telo_exps_cis = {sample: _exp for sample, _exp in results}\n",
    "# # old way of doing it\n",
    "# telo_exps_cis = {k: cooltools.expected_cis( _clr, **exp_kwargs) for k, _clr in telo_clrs.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760aba61",
   "metadata": {},
   "source": [
    "## generate custom kernels - similar to original from hiccups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27dc025-4ae0-4ab8-8615-7c75bf70bfde",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define stripy kernels for small compartment detection ...\n",
    "def get_stripy_kernels_new(halfwidth):\n",
    "    \"\"\"\n",
    "    halfwidth : int\n",
    "        half width of the kernel, kernel size must be odd number in both dimensions\n",
    "\n",
    "    returns :\n",
    "    dictionaty with kernels\n",
    "    \"\"\"\n",
    "    # kernel width defined - odd dimensions ...\n",
    "    kwidth = (2*halfwidth + 1)\n",
    "    # define stripe width\n",
    "    stripe_width = kwidth // 3\n",
    "\n",
    "    # create a grid of coordinates from -h to +h, to define round kernels\n",
    "    x, y = np.meshgrid(\n",
    "        np.linspace(-halfwidth, halfwidth, kwidth),\n",
    "        np.linspace(-halfwidth, halfwidth, kwidth),\n",
    "    )\n",
    "\n",
    "    # define horizontal and vertical stripes\n",
    "    maskv = ((x < stripe_width - halfwidth) | (x > halfwidth - stripe_width))\n",
    "    maskv = maskv & ((y >= stripe_width - halfwidth) & (y <= halfwidth - stripe_width))\n",
    "    maskvmid = ~maskv & ((y >= stripe_width - halfwidth) & (y <= halfwidth - stripe_width))\n",
    "    maskh = ((y < stripe_width - halfwidth) | (y > halfwidth - stripe_width))\n",
    "    maskh = maskh & ((x >= stripe_width - halfwidth) & (x <= halfwidth - stripe_width))\n",
    "    maskhmid = ~maskh & ((x >= stripe_width - halfwidth) & (x <= halfwidth - stripe_width))\n",
    "\n",
    "    # new kernels with more round donut and lowleft masks:\n",
    "    return {\n",
    "        f'mid': maskvmid,\n",
    "        f'v{halfwidth}': maskv,\n",
    "        f'h{halfwidth}': maskh,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17dbc877-fc9c-42c4-b93a-7ae40a2e2294",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define stripy kernels of different sizes\n",
    "k4 = get_stripy_kernels_new(halfwidth=3)\n",
    "k7 = get_stripy_kernels_new(halfwidth=7)  # this is the one we'd need @10kb ...\n",
    "kl = get_stripy_kernels_new(halfwidth=10)\n",
    "\n",
    "\n",
    "# plot rounded kernels\n",
    "fig, axs = plt.subplots(ncols=len(k4), nrows=len([k4, k7, kl]), figsize=(len(k4)*2.5, len([k4, k7, kl])*2.5), squeeze=False)\n",
    "for ax_row, ks in zip(axs, [k4, k7, kl]):\n",
    "    for ax, (ktype, kernel) in zip(ax_row, ks.items()):\n",
    "        imk = draw_kernel(kernel, ax, kernel_name=ktype,cmap=\"plasma\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0e701a-1852-49c0-92ff-d1cd7cbc066a",
   "metadata": {},
   "source": [
    "# Work on a particular clr/exp pair: mostly the dRGap 5hr sample, revisit MEGA dRGap as well ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75443b94-5c4b-4d35-8d69-e99539e822cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "clr = telo_clrs[\"p5hR1R2\"]\n",
    "exp = telo_exps_cis[\"p5hR1R2\"]\n",
    "# got clr, exp and hg38_arms ...\n",
    "# create a special version of exp - indexed by region1,2  and dist:\n",
    "exp_indexed = exp.set_index([\"region1\", \"region2\", \"dist\"]).sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd15c333",
   "metadata": {},
   "source": [
    "### Define all parameters needed for enriched pixels generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48111554",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# binsize - defined\n",
    "max_loci_separation = 30_000_000  # max genomic separation we are going to consider\n",
    "tile_size=10_000_000  # technical paramter for chunking\n",
    "\n",
    "kernel_width = max(len(k) for k in k7.values())  # 2*w+1\n",
    "kernel_half_width = int((kernel_width - 1) / 2)  # former w parameter\n",
    "max_nans_tolerated=11\n",
    "nproc=12\n",
    "\n",
    "# define square tiles (chunks of the heatmap) to run convolutions on ...\n",
    "tiles = list(\n",
    "    generate_tiles_diag_band(\n",
    "        clr,\n",
    "        hg38_arms,\n",
    "        kernel_half_width,\n",
    "        bp_to_bins(tile_size, binsize),  # bp -> bins\n",
    "        bp_to_bins(max_loci_separation, binsize),  # bp -> bins\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1076ccf7-4556-4ff8-b7ac-700eb07c851d",
   "metadata": {},
   "source": [
    "### some interactive exploration of the heatmap tiles ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbe52c8-8260-4b14-aa8e-ccebfd574faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = widgets.IntSlider(value=24, min=0, max=100)\n",
    "\n",
    "@interact(i=w)\n",
    "def f(i):\n",
    "    # selected tile is:\n",
    "    the_tile = tiles[i]\n",
    "    print(\"selected the tile near the region of interest ...\")\n",
    "    print(the_tile)\n",
    "    # display that region of interest as a heatmap ...\n",
    "\n",
    "    # region_name, tile_span_i, tile_span_j = 'chr1_p', (2995, 3505), (2995, 3505)\n",
    "    region_name, tile_span_i, tile_span_j = the_tile\n",
    "    tile_start_ij = (tile_span_i[0], tile_span_j[0])\n",
    "    lazy_exp = LazyToeplitz(\n",
    "        exp_indexed.loc[region_name, region_name][\"balanced.avg\"].to_numpy()\n",
    "    )\n",
    "    # RAW observed matrix slice:\n",
    "    observed = clr.matrix()[slice(*tile_span_i), slice(*tile_span_j)]\n",
    "    expected = lazy_exp[slice(*tile_span_i), slice(*tile_span_j)]\n",
    "\n",
    "    f, (axleft, axright) = plt.subplots(nrows=1,ncols=2,figsize=(24,21),sharex=True,sharey=True)\n",
    "\n",
    "    axleft.imshow(\n",
    "        observed,\n",
    "        cmap=\"fall\",\n",
    "        interpolation=\"none\",\n",
    "        norm=LogNorm(0.0001, 0.01)\n",
    "    )\n",
    "    axright.imshow(\n",
    "        (observed/expected),\n",
    "        cmap=\"RdBu_r\",\n",
    "        interpolation=\"none\",\n",
    "        norm=LogNorm(0.25, 4)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657fe237-4cec-42bb-b4a4-63d5d5850d4d",
   "metadata": {},
   "source": [
    "### Apply some filters for fun - gaussian \"smoothing\", median filter etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7ade06-dab9-4c4d-bfd7-478a0f3745c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "w = widgets.IntSlider(value=24, min=0, max=100)\n",
    "\n",
    "@interact(i=w)\n",
    "def f(i):\n",
    "    # selected tile is:\n",
    "    the_tile = tiles[i]\n",
    "    print(\"selected the tile near the region of interest ...\")\n",
    "    print(the_tile)\n",
    "    # display that region of interest as a heatmap ...\n",
    "\n",
    "    # region_name, tile_span_i, tile_span_j = 'chr1_p', (2995, 3505), (2995, 3505)\n",
    "    region_name, tile_span_i, tile_span_j = the_tile\n",
    "    tile_start_ij = (tile_span_i[0], tile_span_j[0])\n",
    "    lazy_exp = LazyToeplitz(\n",
    "        exp_indexed.loc[region_name, region_name][\"balanced.avg\"].to_numpy()\n",
    "    )\n",
    "    # RAW observed matrix slice:\n",
    "    observed = clr.matrix()[slice(*tile_span_i), slice(*tile_span_j)]\n",
    "    expected = lazy_exp[slice(*tile_span_i), slice(*tile_span_j)]\n",
    "\n",
    "    f, (axleft, axright) = plt.subplots(nrows=1,ncols=2,figsize=(24,21),sharex=True,sharey=True)\n",
    "\n",
    "    OE = (observed/expected)\n",
    "\n",
    "    gOE = scipy.ndimage.gaussian_filter(OE, sigma=1, order=0, mode='reflect', cval=0.0, truncate=3.0)\n",
    "    # gOE = scipy.ndimage.median_filter(OE, size=3, mode='constant', cval=0.0)\n",
    "    # gOE = scipy.ndimage.gaussian_gradient_magnitude(OE, sigma=1, mode='constant', cval=0.0)\n",
    "    # gOE = scipy.ndimage.prewitt(OE, axis=0, mode='constant', cval=0.0)\n",
    "    # gOE = scipy.ndimage.gaussian_gradient_magnitude(OE, sigma=2, mode='reflect', cval=0.0)\n",
    "\n",
    "    axleft.imshow(\n",
    "        gOE,\n",
    "        cmap=\"RdBu_r\",\n",
    "        interpolation=\"none\",\n",
    "        norm=LogNorm(0.25, 4)\n",
    "    )\n",
    "    axright.imshow(\n",
    "        OE,\n",
    "        cmap=\"RdBu_r\",\n",
    "        interpolation=\"none\",\n",
    "        norm=LogNorm(0.25, 4)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d144eed",
   "metadata": {},
   "source": [
    "# Run convolution on the selected tile ...\n",
    " - we use `v7`, `h7`, `mid` kernels\n",
    " - `nnz_threshold` is set to 33% of `k_sum.sum()` (tried 40% before)\n",
    " - `enrichment_threshold` is set to **2**\n",
    " - `max_nans_tolerated` is **11**\n",
    " - `max_loci_separation` is at `30_000_000`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab602ba1-e87d-4f8a-bb35-b404aa564567",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# columns to return\n",
    "cols_to_return = [\"bin1_id\", \"bin2_id\", \"count\"] #, \"expected_raw\", \"oe\"]\n",
    "cols_to_return += [f\"la_exp.{k}.value\" for k in k7]\n",
    "cols_to_return += [f\"convolution_ratio.{k}\" for k in k7]\n",
    "cols_to_return += [f\"la_exp.{k}.zeros\" for k in k7]\n",
    "\n",
    "def extract_filtered_pixels(df, enrichment_threshold, nnz_threshold):\n",
    "    # make df more lightweight - drop a few columns\n",
    "    cols_to_drop = [f\"la_exp.{k}.value\" for k in k7]\n",
    "    df = df.drop(columns=cols_to_drop, inplace=False, errors='raise')\n",
    "    # first define a few extra columns etc\n",
    "    df[\"dist\"] = df[\"bin2_id\"] - df[\"bin1_id\"]\n",
    "    df[\"zl\"] = df[[f\"la_exp.{k}.zeros\" for k in [\"v7\",\"h7\",\"mid\"]]].sum(axis=1)\n",
    "\n",
    "    # filtering queries:\n",
    "    query_v = f\"(`convolution_ratio.mid` >= {enrichment_threshold} * `convolution_ratio.v7`)\"\n",
    "    query_h = f\"(`convolution_ratio.mid` >= {enrichment_threshold} * `convolution_ratio.h7`)\"\n",
    "\n",
    "    return df.query(f\"(zl <= {nnz_threshold}) & ( {query_v} | {query_h} )\"\n",
    "    )\n",
    "\n",
    "\n",
    "def score_pixels_OE_bulk(\n",
    "    clr,\n",
    "    expected_indexed,\n",
    "    expected_value_col,\n",
    "    clr_weight_name,\n",
    "    tiles,\n",
    "    kernels,\n",
    "    max_nans_tolerated,\n",
    "    loci_separation_bins,\n",
    "    nproc,\n",
    "    cols_to_return = None,\n",
    "):\n",
    "    logging.info(f\"convolving {len(tiles)} tiles to build histograms for lambda-bins\")\n",
    "\n",
    "    # ########################################################\n",
    "    # these are important thresholding parameters right here !\n",
    "    # ########################################################\n",
    "    _enrichment_threshold = 2.0\n",
    "    k_sum = sum([k.astype(int) for n,k in kernels.items() if n in [\"v7\",\"h7\",\"mid\"]])\n",
    "    _nnz_threshold = 0.33 * k_sum.sum()\n",
    "\n",
    "    # to score per tile - a function of a single argument :\n",
    "    _score = partial(\n",
    "        score_tile_custom_cols_OE,\n",
    "        clr=clr,\n",
    "        expected_indexed=expected_indexed,\n",
    "        expected_value_col=expected_value_col,\n",
    "        clr_weight_name=clr_weight_name,\n",
    "        kernels=kernels,\n",
    "        max_nans_tolerated=max_nans_tolerated,\n",
    "        band_to_cover=loci_separation_bins,\n",
    "        cols_to_return=cols_to_return,\n",
    "    )\n",
    "    #\n",
    "    _filter = partial(\n",
    "        extract_filtered_pixels,\n",
    "        enrichment_threshold=_enrichment_threshold,\n",
    "        nnz_threshold=_nnz_threshold,\n",
    "    )\n",
    "    # ...\n",
    "    # compose scoring and histogramming together\n",
    "    _job = lambda tile: _filter(_score(tile))\n",
    "    # standard multiprocessing implementation\n",
    "    if nproc > 1:\n",
    "        logging.info(f\"creating a Pool of {nproc} workers to tackle {len(tiles)} tiles\")\n",
    "        pool = mp.Pool(nproc)\n",
    "        map_ = pool.imap\n",
    "        map_kwargs = dict(chunksize=int(np.ceil(len(tiles) / nproc)))\n",
    "    else:\n",
    "        logging.info(\"fallback to serial implementation.\")\n",
    "        map_ = map\n",
    "        map_kwargs = {}\n",
    "    try:\n",
    "        scored_df_chunks = map_(_job, tiles, **map_kwargs)\n",
    "    finally:\n",
    "        if nproc > 1:\n",
    "            pool.close()\n",
    "\n",
    "    return scored_df_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5692c8b-02a3-4bde-bca2-38d0defbf1e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run pixels scoring and extraction/filtering on each tile ...\n",
    "scored_df = score_pixels_OE_bulk(\n",
    "    clr=clr,\n",
    "    expected_indexed=exp_indexed,\n",
    "    expected_value_col=\"balanced.avg\",\n",
    "    clr_weight_name=\"weight\",\n",
    "    tiles=tiles,\n",
    "    kernels=k7,  # using k7 @10kb\n",
    "    max_nans_tolerated=max_nans_tolerated,\n",
    "    loci_separation_bins=bp_to_bins(max_loci_separation, binsize),\n",
    "    nproc=12,\n",
    "    cols_to_return=cols_to_return,\n",
    ")\n",
    "# stitch returned enriched pixels from each tile ...\n",
    "enriched_pixels = pd.concat(scored_df, ignore_index=True)\n",
    "\n",
    "# report ...\n",
    "print(f\"detected {len(enriched_pixels)} ...\")\n",
    "\n",
    "if False:\n",
    "    # save enriched pixels for plotting/exploration and such ...\n",
    "    enriched_pixels.to_csv(\n",
    "        \"enriched_pixels_10kb/5hr_2X_aug24.binpe\",\n",
    "        sep=\"\\t\",\n",
    "        index=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f656e5a-1142-4747-9815-73455654f90a",
   "metadata": {},
   "source": [
    "# Now we have a list of enriched pixels: `enriched_pixels`\n",
    " - bin coordinates only\n",
    " - with some extra info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eed8aa3-7202-43f5-b067-a69736603358",
   "metadata": {},
   "source": [
    "# Density-cluster `enriched_pixels`\n",
    " - go OPTICS, consider DBSCAN ...\n",
    " - minimal cluster size is `5`\n",
    " - clustering radius is `33_000` bp ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cabc332-f5e1-4306-bc5b-553a741d4d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# annotate enriched pixels with chrom, start, end info ...\n",
    "enriched_pixels_annotated = cooler.annotate(\n",
    "    enriched_pixels,\n",
    "    clr.bins()[[\"chrom\", \"start\", \"end\"]],\n",
    "    replace=False\n",
    ")\n",
    "# Clustering is done independently for every region, therefore regions must be assigned:\n",
    "enriched_pixels_annotated = assign_regions(enriched_pixels_annotated, hg38_arms)\n",
    "# make regions categorical - such that sorting is preserved ...\n",
    "enriched_pixels_annotated[\"region\"] = pd.Categorical(enriched_pixels_annotated[\"region\"], categories = included_arms)\n",
    "\n",
    "# let's try density based clustering here ...\n",
    "# we are working in cis strictly here - so just region is enough:\n",
    "grp = enriched_pixels_annotated.groupby(\"region\")\n",
    "\n",
    "cols_rename = {\n",
    "    ('chrom1', 'first'): \"chrom1\",\n",
    "    ('start1', 'min'): \"start1\",\n",
    "    ('end1', 'max'): \"end1\",\n",
    "    ('chrom2', 'first'): \"chrom2\",\n",
    "    ('start2', 'min'): \"start2\",\n",
    "    ('end2', 'max'): \"end2\",\n",
    "    ('bin1_id', 'min'): \"bin1_id\",\n",
    "    ('bin1_id', 'ptp'): \"bin1_width\",\n",
    "    ('bin2_id', 'min'): \"bin2_id\",\n",
    "    ('bin2_id', 'ptp'): \"bin2_width\",\n",
    "    ('count', 'sum'): \"count\",\n",
    "    ('dist', 'min'): \"dist\",\n",
    "    ('zl', 'sum'): \"nnz\",\n",
    "    ('region', 'first'): \"region\",\n",
    "    ('region', 'count'): \"cluster_size\",\n",
    "}\n",
    "\n",
    "_min_cluster_size = 5\n",
    "_clustering_radius = 33_000  # in basepairs ...\n",
    "_clustered_pix_region = []\n",
    "\n",
    "for grp_region, df in grp:\n",
    "    # cluster by region ...\n",
    "    print(f\"clustering {grp_region} ...\")\n",
    "    if len(df):\n",
    "        _clust = OPTICS(\n",
    "            max_eps=_clustering_radius,\n",
    "            min_samples=_min_cluster_size\n",
    "        ).fit( df[[\"start1\",\"start2\"]].to_numpy() )\n",
    "        # assign labels back to the dataframe (per region) ...\n",
    "        _cluster_labels = pd.Series(_clust.labels_).astype(\"str\")\n",
    "        df[\"labels\"] = ( f\"{grp_region}_\" + _cluster_labels ).to_numpy()\n",
    "        # filter out singletons ...\n",
    "        df = df[ ~df[\"labels\"].str.endswith(\"-1\") ]\n",
    "        # group by clustering labels ...\n",
    "        grp = df.groupby(by=\"labels\")\n",
    "        chrdf = grp.agg({\n",
    "            \"chrom1\" : \"first\",\n",
    "            \"start1\" : \"min\",\n",
    "            \"end1\" : \"max\",\n",
    "            \"chrom2\" : \"first\",\n",
    "            \"start2\" : \"min\",\n",
    "            \"end2\" : \"max\",\n",
    "            \"bin1_id\" : [\"min\", np.ptp],\n",
    "            \"bin2_id\" : [\"min\", np.ptp],\n",
    "            \"count\" : \"sum\",\n",
    "            \"dist\" : \"min\",\n",
    "            \"zl\" : \"sum\",\n",
    "            \"region\" : [\"first\", \"count\"],\n",
    "        })\n",
    "        chrdf.columns = chrdf.columns.to_flat_index()\n",
    "        chrdf = chrdf.rename(columns=cols_rename).reset_index()\n",
    "        _clustered_pix_region.append(chrdf)\n",
    "    else:\n",
    "        print(f\"WARNING: {grp_region} region is empty, skipping ...\")\n",
    "\n",
    "\n",
    "# concat regions together ...\n",
    "clustered_pixels = pd.concat(_clustered_pix_region, ignore_index=True)\n",
    "print(f\"\\nClustering yields {len(clustered_pixels)} pixel-clusters ...\")\n",
    "\n",
    "if False:\n",
    "    clustered_pixels.to_csv(\n",
    "        \"clustered_pixels_10kb/5hr_2X_aug24.binpe\",\n",
    "        sep=\"\\t\",\n",
    "        index=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1a5296-2fdc-4956-9132-cb490e8eecd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered_pixels[\"dist\"].hist(bins=100, log=False)\n",
    "ax = plt.gca()\n",
    "ax.set_xlabel(\"ID interactions genomic separation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97718d08-c9dd-4675-8ba8-3a0f771745d5",
   "metadata": {},
   "source": [
    "# Cluster, keeping individual pixels there [removing singletons]...\n",
    " - an attemp to improve accuracy of anchor calling ...\n",
    " - i.e. make only \"cloud-forming\" pixels participate in the anchor formation\n",
    " - remove singletons altogether ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6856434b-6e65-45ad-b236-3d5772c23b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "_clustered_pixels_all = []\n",
    "\n",
    "# let's try density based clustering here ...\n",
    "# we are working in cis strictly here - so just region is enough:\n",
    "grp = enriched_pixels_annotated.groupby(\"region\", observed=True)\n",
    "for grp_region, df in grp:\n",
    "    # cluster by region ...\n",
    "    if len(df):\n",
    "        print(f\"clustering {len(df)} pixels from {grp_region} ...\")\n",
    "        _clust = OPTICS(\n",
    "            max_eps=_clustering_radius,\n",
    "            min_samples=_min_cluster_size\n",
    "        ).fit(\n",
    "            df.eval(\n",
    "                \"\"\"\n",
    "                x = (start1 + end1)*0.5\n",
    "                y = (start2 + end2)*0.5\n",
    "                \"\"\"\n",
    "            )[[\"x\",\"y\"]].to_numpy()\n",
    "        )\n",
    "        # assign labels back to the dataframe (per region) ...\n",
    "        _cluster_labels = pd.Series(_clust.labels_).astype(\"str\")\n",
    "        df[\"labels\"] = ( f\"{grp_region}_\" + _cluster_labels ).to_numpy()\n",
    "        # filter out singletons ...\n",
    "        df = df[ ~df[\"labels\"].str.endswith(\"-1\") ]\n",
    "        _clustered_pixels_all.append(df)\n",
    "    else:\n",
    "        print(f\"WARNING: {grp_region} region is empty, skipping ...\")\n",
    "\n",
    "# # concat and assign ..\n",
    "# enriched_pixels_annotated[\"labels\"] = pd.concat(_labels).to_numpy()\n",
    "clustered_pixels_all = pd.concat(_clustered_pixels_all, ignore_index=True)\n",
    "print(f\"\\nClustering yields {len(clustered_pixels_all)} pixel-clusters ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b5255b-505a-4345-aa7a-665b8e23fd32",
   "metadata": {},
   "source": [
    "# Save `enriched_pixels` and `clustered_pixels` on disk ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061e28ef-7658-412a-99fe-fbaba552eb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...\n",
    "dirname = \"enriched_pixels_10kb\"\n",
    "fname = f\"{dirname}/5hr_2X_second.binpe\"\n",
    "print(f\"saving {len(enriched_pixels)} enriched pixels to {fname} in BEDPE format (only bin_ids actually @10kb) ...\")\n",
    "enriched_pixels.to_csv(fname, sep=\"\\t\", index=False)\n",
    "# ...\n",
    "# w/o singletons\n",
    "fname = f\"{dirname}/5hr_2X_second.no_singletons.binpe\"\n",
    "print(f\"saving {len(clustered_pixels_all)} enriched pixels (w/o singletons) to {fname} in BEDPE format (only bin_ids actually @10kb) ...\")\n",
    "clustered_pixels_all.to_csv(fname, sep=\"\\t\", index=False)\n",
    "\n",
    "# ...\n",
    "dirname = \"native_comps_10kb\"\n",
    "fname = f\"{dirname}/5hr_2X_second.bedpe\"\n",
    "print(f\"saving {len(clustered_pixels)} clustered pixels to {fname} in BEDPE format ...\")\n",
    "clustered_pixels.to_csv(fname, sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb70117-a98a-4039-bd03-169a1a363ee0",
   "metadata": {},
   "source": [
    "# Enriched pixels coverage \"games\" to define anchors ...\n",
    " - define coverage of enriched pixels (and enriched ones w/o singletons)\n",
    " - check out its distributions and example tracks (save it is bigwig as well) ...\n",
    " - use `scipy.signal.find_peaks` to detect anchors and their footprint\n",
    " - `_val_thresh = 7` - is the bottom floor for the coverage of enriched pixels ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce4a927-fbd3-4194-a7e2-e6fbb679d19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bin_coverage(df):\n",
    "    \"\"\"\n",
    "    df with bin1_id and bin2_id columns\n",
    "    coverage for every bin that is out there ...\n",
    "    \"\"\"\n",
    "    # simply count \"valencies\" of enriched pixels i-s and j-s - and sum tham up togeher ...\n",
    "    b1cov = df.groupby(\"bin1_id\").size()\n",
    "    b2cov = df.groupby(\"bin2_id\").size()\n",
    "    b1cov.index.name = \"bin\"\n",
    "    b2cov.index.name = \"bin\"\n",
    "    return b1cov.add(b2cov, fill_value=0)\n",
    "\n",
    "def calculate_valencies(\n",
    "    bed_df,  # must be output of bedpe_to_anchors, which in turn is a clustering inside\n",
    "    bedpe_df,\n",
    "    cluster_colname = \"cluster\",\n",
    "    valency_colname = \"valency\",\n",
    "    bed_cols = [\"chrom\", \"cluster_start\", \"cluster_end\"],\n",
    "    bedpe_cols1 = [\"chrom1\", \"start1\", \"end1\"],\n",
    "    bedpe_cols2 = [\"chrom2\", \"start2\", \"end2\"],\n",
    "):\n",
    "    \"\"\"\n",
    "    calculate valencies of a given anchors, given the bedpe ...\n",
    "    \"\"\"\n",
    "    if cluster_colname not in bed_df.columns:\n",
    "        raise ValueError(\"bed_df does not seem to be the result of bedpe_to_anchors/clustering ...\")\n",
    "    # overlap combined anchors with the left anchors to see how many \"dots\" we overlap ...\n",
    "    anchors_left = bioframe.overlap(\n",
    "        bed_df,\n",
    "        bedpe_df,\n",
    "        how='left',\n",
    "        cols1=bed_cols,\n",
    "        cols2=bedpe_cols1,\n",
    "    ).dropna( subset=[f\"{c}_\" for c in bedpe_cols1] )\n",
    "    # overlap combined anchors with the right anchors to see how many \"dots\" we overlap ...\n",
    "    anchors_right = bioframe.overlap(\n",
    "        bed_df,\n",
    "        bedpe_df,\n",
    "        how='left',\n",
    "        cols1=bed_cols,\n",
    "        cols2=bedpe_cols2,\n",
    "    ).dropna( subset=[f\"{c}_\" for c in bedpe_cols2] )\n",
    "    _num_clusters = len(bed_df)\n",
    "    # sanity check here ... - make sure we cover all of the cluster that are available ...\n",
    "    assert ( bed_df[cluster_colname].sort_values() == np.arange(_num_clusters) ).all()\n",
    "    # ...\n",
    "    _empty_clust_series = pd.Series(\n",
    "        data=np.zeros(_num_clusters),\n",
    "        index=pd.Index(data=np.arange(_num_clusters), name=cluster_colname),\n",
    "        name=\"count\"\n",
    "    )\n",
    "    # calculate valencies ...\n",
    "    _valencies = (_empty_clust_series + anchors_left[cluster_colname].value_counts()).fillna(0) \\\n",
    "                + (_empty_clust_series + anchors_right[cluster_colname].value_counts()).fillna(0)\n",
    "    # assign valencies back to anchors bed_df - carefully !\n",
    "    # _valencies are indexed using cluster_id - s\n",
    "    bed_df_clust_indexed = bed_df.set_index(cluster_colname)\n",
    "    bed_df_clust_indexed[valency_colname] = _valencies.astype(int)\n",
    "    #\n",
    "    return bed_df_clust_indexed.reset_index()\n",
    "\n",
    "\n",
    "def coverage_to_anchors(cov_df, full_bintable, coverage_threshold, clustered_pixels=None):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # _idx = rich_pix_vals[rich_pix_vals > _val_threshold].index\n",
    "    _rich_bins = full_bintable.loc[cov_df[cov_df > coverage_threshold].index]\n",
    "\n",
    "    _new_anchors = bioframe.merge(_rich_bins)\n",
    "    _new_anchors = _new_anchors.reset_index().rename(columns={\"index\":\"cluster\"})\n",
    "    # add few extra columns and save it ...\n",
    "    _new_anchors[\"size\"] = _new_anchors.eval(\"end - start\")\n",
    "    if clustered_pixels is not None:\n",
    "        # calculate valency the old way by overlapping with actually clustered pixels ...\n",
    "        _new_anchors = calculate_valencies(\n",
    "            _new_anchors,  # must be output of bedpe_to_anchors, which in turn is a clustering inside\n",
    "            clustered_pixels,\n",
    "            cluster_colname = \"cluster\",\n",
    "            valency_colname = \"valency\",\n",
    "            bed_cols = [\"chrom\", \"start\", \"end\"],\n",
    "            bedpe_cols1 = [\"chrom1\", \"start1\", \"end1\"],\n",
    "            bedpe_cols2 = [\"chrom2\", \"start2\", \"end2\"],\n",
    "        )\n",
    "    else:\n",
    "        _new_anchors[\"valency\"] = 10\n",
    "    # return ...\n",
    "    return _new_anchors\n",
    "\n",
    "\n",
    "# calculate pixels coverage ...\n",
    "rich_pix_vals = get_bin_coverage(enriched_pixels)\n",
    "rich_clust_pix_vals = get_bin_coverage(clustered_pixels_all)\n",
    "\n",
    "# take empty bins and fill non-zero coverage with rich_clust_pix_vals ...\n",
    "# use value threshold to define the \"floor\" of the pixel coverage ...\n",
    "_val_thresh = 7\n",
    "\n",
    "_bins = clr.bins()[:]\n",
    "_bins.index.name = \"bin\"\n",
    "_bins[\"cov\"] = rich_clust_pix_vals\n",
    "_bins[\"cov\"] = _bins[\"cov\"].fillna(0)\n",
    "_arr = _bins[\"cov\"].to_numpy()\n",
    "# detect praks on the coverage track ...\n",
    "_peaks, _props = find_peaks(\n",
    "    np.clip(_arr, _val_thresh, None),\n",
    "    prominence=(None,None),\n",
    "    distance=5,\n",
    ")\n",
    "print(f\"{len(_peaks)} peaks were called ...\")\n",
    "# extract left/right boundaries of every peak ...\n",
    "_lefts = _props[\"left_bases\"]\n",
    "_rights = _props[\"right_bases\"]\n",
    "_arr_clipped = np.clip(_arr, _val_thresh, None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7e9ab7-eb71-467e-8909-c903994260ff",
   "metadata": {},
   "source": [
    "### save coverage track as bigWig ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a835f022-96c9-45e4-8e8c-c602741bae2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bw_fname = \"pix_clust_cov.bw\"\n",
    "# save that coverage track as bigwig ...\n",
    "import pybigtools\n",
    "_bins_tobw = _bins.copy()\n",
    "_bins_tobw[\"chrom\"] = _bins_tobw[\"chrom\"].astype(\"str\")\n",
    "_bins_tobw = _bins_tobw[[\"chrom\", \"start\", \"end\", \"cov\"]]\n",
    "bw = pybigtools.open(bw_fname, mode=\"w\")\n",
    "bw.write(\n",
    "    hg38_chromsizes.to_dict(),\n",
    "    iter(tuple(t) for t in _bins_tobw.sort_values(by=[\"chrom\",\"start\"]).itertuples(index=False)),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d91f96b-cf5c-4989-807c-42ea26f6a5e6",
   "metadata": {},
   "source": [
    "## Visualize the anchor detection procdeure ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01935339-4130-48ee-8359-9f11a9256e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,10), layout=\"tight\")\n",
    "gs = GridSpec(3, 2, figure=fig)\n",
    "ax1 = fig.add_subplot(gs[0,0])\n",
    "ax2 = fig.add_subplot(gs[0,1])\n",
    "ax3 = fig.add_subplot(gs[1,:])\n",
    "ax4 = fig.add_subplot(gs[2,:])\n",
    "fig.suptitle(\"enriched pixels coverage exploration, aka 'valency'\")\n",
    "\n",
    "_max_val = 200\n",
    "ax1.hist(rich_pix_vals.to_numpy(), bins=np.linspace(0,_max_val,_max_val+1), log=True);\n",
    "ax1.hist(rich_clust_pix_vals.to_numpy(), bins=np.linspace(0,_max_val,_max_val+1), log=True, alpha=0.88);\n",
    "ax1.axvline(_val_thresh,color=\"red\")\n",
    "ax1.set_xlabel(\"valency\")\n",
    "ax1.set_ylabel(\"hist. counts\")\n",
    "ax1.set_xlim(0, _max_val)\n",
    "_max_val = 30\n",
    "ax2.hist(rich_pix_vals.to_numpy(), bins=np.linspace(0,_max_val,_max_val+1), log=True);\n",
    "ax2.hist(rich_clust_pix_vals.to_numpy(), bins=np.linspace(0,_max_val,_max_val+1), log=True, alpha=0.88);\n",
    "ax2.axvline(_val_thresh,color=\"red\")\n",
    "ax2.set_xlabel(\"valency\")\n",
    "ax2.set_title(\"hist zoom in\")\n",
    "ax2.set_xlim(0, _max_val)\n",
    "\n",
    "# example of coverage track and stuff within a region ...\n",
    "_chrom = \"chr6\"\n",
    "_reg_start = 127_000_000\n",
    "_reg_length = 10_000_000\n",
    "_from, _to = clr.extent((_chrom, _reg_start, _reg_start+_reg_length))\n",
    "# print(_from, _to)\n",
    "rich_pix_vals_plot = pd.Series(index=np.arange(_from,_to)).add(rich_pix_vals.loc[_from: _to], fill_value=0)\n",
    "rich_clust_pix_vals_plot = pd.Series(index=np.arange(_from,_to)).add(rich_clust_pix_vals.loc[_from: _to], fill_value=0)\n",
    "\n",
    "ax3.plot(rich_pix_vals_plot, marker=\".\", lw=0.5, label=\"all enriched pixels\")\n",
    "ax3.plot(rich_clust_pix_vals_plot, marker=\".\", lw=0.5, label=\"all enriched pixels (w/o singletons)\")\n",
    "ax3.axhline(_val_thresh, color=\"red\")\n",
    "ax3.set_xlim(_from, _to)\n",
    "ax3.legend(frameon=False)\n",
    "ax3.set_title(f\"example coverage: {_chrom}:{_reg_start}-{_reg_start+_reg_length}\")\n",
    "\n",
    "ax4.plot(_arr_clipped[_from: _to])\n",
    "ax4.plot(_peaks - _from, _arr_clipped[_peaks], \"x\" )\n",
    "ax4.plot(_lefts - _from, _arr_clipped[_lefts], \"o\" )\n",
    "ax4.plot(_rights - _from, _arr_clipped[_rights], \"o\" )\n",
    "ax4.set_xlim(0, _to-_from )\n",
    "ax4.set_title(f\"clipped coverage with detected anchors/peaks ...\")\n",
    "# ax.axhline(_val_thresh, color=\"red\")\n",
    "\n",
    "\n",
    "# try adding an axes manually ...\n",
    "fig.add_axes([0.88,0.01,0.1,0.02])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0d26f0-2b8e-4d45-b3e7-e258b40157c0",
   "metadata": {},
   "source": [
    "## Anchors from peaks (clipped coverage way) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175bf204-9b63-4fa1-94fb-f854b31811a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "_new_new_anchors = coverage_to_anchors(\n",
    "    pd.Series(data=10., index=_peaks),\n",
    "    clr.bins()[:],\n",
    "    0,\n",
    "    clustered_pixels=clustered_pixels\n",
    ")\n",
    "# modify their size to the actual footprint of the peak ...\n",
    "# _new_new_anchors[\"size\"] = _props[\"right_bases\"] - _props[\"left_bases\"]\n",
    "_new_new_anchors[\"peak_start\"] = clr.bins()[:].loc[_lefts, \"start\"].to_numpy()\n",
    "_new_new_anchors[\"peak_end\"] = clr.bins()[:].loc[_rights, \"end\"].to_numpy()\n",
    "# ...\n",
    "_new_new_anchors[\"size\"] = _new_new_anchors.eval(\"peak_end - peak_start\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faaac6ed-bf00-49ec-b690-525a980e5946",
   "metadata": {},
   "outputs": [],
   "source": [
    "_new_new_anchors[\"size\"].hist(bins=30)\n",
    "ax = plt.gca()\n",
    "ax.set_xlabel(\"ID anchor footprint, bp\")\n",
    "_new_new_anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8a6c3e-d7e4-46cd-bebd-efa13a6a23ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save it ...\n",
    "_cols_save = [\"chrom\", \"start\", \"end\", \"cluster\", \"size\", \"valency\", \"peak_start\", \"peak_end\", \"size\"]\n",
    "dirname = \"ID_anchors\"\n",
    "fname = f\"{dirname}/5hr_2X_enrichment.pixel_derived.signal_peaks.bed\"\n",
    "_new_new_anchors[_cols_save].to_csv(\n",
    "    fname,\n",
    "    index=False,\n",
    "    sep=\"\\t\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8827f867-1d72-4322-9c6c-fc9e6f7ae3b4",
   "metadata": {},
   "source": [
    "## Other anchors ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94467a54-954f-436e-b495-5400fc7ce11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "_new_anchors = coverage_to_anchors(\n",
    "    rich_pix_vals,\n",
    "    clr.bins()[:],\n",
    "    _val_thresh,\n",
    "    clustered_pixels=clustered_pixels\n",
    ")\n",
    "\n",
    "# save it ...\n",
    "_cols_save = [\"chrom\",\"start\",\"end\",\"cluster\",\"size\",\"valency\"]\n",
    "dirname = \"ID_anchors\"\n",
    "fname = f\"{dirname}/5hr_2X_enrichment.pixel_derived.bed\"\n",
    "_new_anchors[_cols_save].to_csv(\n",
    "    fname,\n",
    "    index=False,\n",
    "    sep=\"\\t\",\n",
    ")\n",
    "\n",
    "_new_anchors_nosing = coverage_to_anchors(\n",
    "    rich_clust_pix_vals,\n",
    "    clr.bins()[:],\n",
    "    _val_thresh,\n",
    "    clustered_pixels=clustered_pixels\n",
    ")\n",
    "\n",
    "# save it ...\n",
    "_cols_save = [\"chrom\",\"start\",\"end\",\"cluster\",\"size\",\"valency\"]\n",
    "dirname = \"ID_anchors\"\n",
    "fname = f\"{dirname}/5hr_2X_enrichment.pixel_derived.no_singletons.bed\"\n",
    "_new_anchors_nosing[_cols_save].to_csv(\n",
    "    fname,\n",
    "    index=False,\n",
    "    sep=\"\\t\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bff15b-93d9-4edb-8238-a875e99198a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0951eb46-0292-438c-bf55-861b0685521e",
   "metadata": {},
   "source": [
    "# Now define some plotting functions and draw enriched pixels and clusters on heatmaps ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e06e118-2f8b-4300-aa50-cad109df8078",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rectangles_around_dots(dots_bins_df, the_tile, loc=\"upper\", lw=1, ec=\"cyan\", fc=\"none\"):\n",
    "    rectangle_kwargs = dict(lw=lw, ec=ec, fc=fc)\n",
    "    # parse the tile\n",
    "    _, tspan1, tspan2 = the_tile\n",
    "    # select only visible pixels :\n",
    "    _the_dots = dots_bins_df \\\n",
    "        .query(\"\"\"(@tspan1[0] < bin1_id < @tspan1[1]) & \\\n",
    "                  (@tspan2[0] < bin2_id < @tspan2[1]) \"\"\") \\\n",
    "        .eval(\"\"\"b1 = bin1_id - @tspan1[0]\n",
    "                 b2 = bin2_id - @tspan2[0] \"\"\")\n",
    "    print(f\"{len(_the_dots)} pixels are visible out of {len(dots_bins_df)} ...\")\n",
    "    # iterate over visible pixels...\n",
    "    for b1, b2 in _the_dots[[\"b1\", \"b2\"]].itertuples(index=False):\n",
    "        w1 = w2 = 1\n",
    "        if loc == \"upper\":\n",
    "            yield patches.Rectangle((b2, b1), w2, w1, **rectangle_kwargs)\n",
    "        elif loc == \"lower\":\n",
    "            yield patches.Rectangle((b1, b2), w1, w2, **rectangle_kwargs)\n",
    "        else:\n",
    "            raise ValueError(\"loc has to be uppper or lower\")\n",
    "\n",
    "# in a specific region, and exposing importnat plotting parameters\n",
    "def rectangles_around_dots_ww(dots_bins_df, the_tile, loc=\"upper\", lw=1, ec=\"cyan\", fc=\"none\", halo=30_000):\n",
    "    rectangle_kwargs = dict(lw=lw, ec=ec, fc=fc)\n",
    "    # parse the tile\n",
    "    _, tspan1, tspan2 = the_tile\n",
    "    # select only visible \"boxes\" :\n",
    "    _the_dots = dots_bins_df \\\n",
    "        .query(\"\"\"(@tspan1[0] - @halo < bin1_id < @tspan1[1] + @halo) & \\\n",
    "                  (@tspan2[0] - @halo < bin2_id < @tspan2[1] + @halo) \"\"\") \\\n",
    "        .eval(\"\"\"b1 = bin1_id - @tspan1[0]\n",
    "                 b2 = bin2_id - @tspan2[0] \"\"\")\n",
    "    print(f\"{len(_the_dots)} pixels are visible out of {len(dots_bins_df)} ...\")\n",
    "    for b1, b2, w1, w2 in _the_dots[[\"b1\", \"b2\", \"bin1_width\", \"bin2_width\"]].itertuples(index=False):\n",
    "        if loc == \"upper\":\n",
    "            yield patches.Rectangle((b2, b1), w2+1, w1+1, **rectangle_kwargs)\n",
    "        elif loc == \"lower\":\n",
    "            yield patches.Rectangle((b1, b2), w1+1, w2+1, **rectangle_kwargs)\n",
    "        else:\n",
    "            raise ValueError(\"loc has to be uppper or lower\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236f2a0e-d381-49e2-a0fe-f962ab893b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display that region of interest as a heatmap ...\n",
    "i_slider = widgets.IntSlider(min=0, max=100, step=1, value=24, description=\"tile id :\")\n",
    "\n",
    "@interact(i=i_slider)\n",
    "def f(i):\n",
    "    # region_name, tile_span_i, tile_span_j = 'chr1_p', (2995, 3505), (2995, 3505)\n",
    "    _tile_id = i\n",
    "    _the_tile = tiles[_tile_id]\n",
    "    # for _tile_id, _the_tile in enumerate(tiles[69:], start=69):\n",
    "    region_name, tile_span_i, tile_span_j = _the_tile\n",
    "    tile_start_ij = (tile_span_i[0], tile_span_j[0])\n",
    "    lazy_exp = LazyToeplitz(\n",
    "        exp_indexed.loc[region_name, region_name][\"balanced.avg\"].to_numpy()\n",
    "    )\n",
    "    # RAW observed matrix slice:\n",
    "    observed = clr.matrix()[slice(*tile_span_i), slice(*tile_span_j)]\n",
    "    expected = lazy_exp[slice(*tile_span_i), slice(*tile_span_j)]\n",
    "\n",
    "    # let's figure out slices' coordinates ....\n",
    "    _bins_i = clr.bins()[slice(*tile_span_i)]\n",
    "    _bins_j = clr.bins()[slice(*tile_span_j)]\n",
    "    _chrom_i, _start_i, _end_i = _bins_i.iloc[0][\"chrom\"], _bins_i.iloc[0][\"start\"], _bins_i.iloc[-1][\"end\"]\n",
    "    _chrom_j, _start_j, _end_j = _bins_j.iloc[0][\"chrom\"], _bins_j.iloc[0][\"start\"], _bins_j.iloc[-1][\"end\"]\n",
    "\n",
    "\n",
    "    f, (axleft, axright) = plt.subplots(nrows=1,ncols=2,figsize=(24,11),sharex=True,sharey=True)\n",
    "    f.suptitle(f\"tile # {_tile_id} {(_chrom_i, _start_i, _end_i)} {(_chrom_j, _start_j, _end_j)}\",y=0.9)\n",
    "\n",
    "    print(f\"tile # {_tile_id} {(_chrom_i, _start_i, _end_i)} {(_chrom_j, _start_j, _end_j)}\")\n",
    "\n",
    "    axleft.imshow(\n",
    "        observed,\n",
    "        cmap=\"fall\",\n",
    "        interpolation=\"none\",\n",
    "        norm=LogNorm(0.0001, 0.01)\n",
    "    )\n",
    "    axright.imshow(\n",
    "        (observed/expected),\n",
    "        cmap=\"RdBu_r\",\n",
    "        interpolation=\"none\",\n",
    "        norm=LogNorm(0.25, 4)\n",
    "    )\n",
    "\n",
    "    # draw rectangular \"boxes\" around enriched pixels ...\n",
    "    for box in rectangles_around_dots(\n",
    "        enriched_pixels,\n",
    "        _the_tile,\n",
    "        loc=\"upper\",\n",
    "        lw=1,\n",
    "        ec=\"cyan\",\n",
    "        fc=\"none\"\n",
    "    ):\n",
    "        axleft.add_patch(box)\n",
    "\n",
    "    # draw rectangular \"boxes\" around clustered pixels ...\n",
    "    for box in rectangles_around_dots_ww(\n",
    "        clustered_pixels,\n",
    "        _the_tile,\n",
    "        loc=\"upper\",\n",
    "        lw=1,\n",
    "        ec=\"k\",\n",
    "        fc=\"none\",\n",
    "        halo=100,\n",
    "    ):\n",
    "        axleft.add_patch(box)\n",
    "    for box in rectangles_around_dots_ww(\n",
    "        clustered_pixels,\n",
    "        _the_tile,\n",
    "        loc=\"upper\",\n",
    "        lw=1,\n",
    "        ec=\"darkgreen\",\n",
    "        fc=\"none\",\n",
    "        halo=100\n",
    "    ):\n",
    "        axright.add_patch(box)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd9865e-b1fe-42fb-92b2-8291bd989149",
   "metadata": {},
   "source": [
    "# Below is a sanity Pileup exploration of the `clustered_pixels`\n",
    " - all\n",
    " - by distance\n",
    " - by distance and chromosome/arm\n",
    " - by cluster size ...\n",
    "\n",
    "### Goal is to demonstrate that the \"calls\" are not spurious and look \"tight\" individually as well ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a030a7-7f2b-482b-a985-32f29000b364",
   "metadata": {},
   "outputs": [],
   "source": [
    "_flank = 100_000 # Length of flank to one side from the boundary, in basepairs\n",
    "# create the stack of snips:\n",
    "fullstack = cooltools.pileup(\n",
    "    clr,\n",
    "    clustered_pixels,\n",
    "    view_df=hg38_arms,\n",
    "    expected_df=exp,\n",
    "    flank=_flank,\n",
    "    nproc=12\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6616f82c-70cf-485f-9f82-79de805d039e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-21T15:16:37.553647Z",
     "iopub.status.busy": "2023-09-21T15:16:37.553259Z",
     "iopub.status.idle": "2023-09-21T15:16:37.556129Z",
     "shell.execute_reply": "2023-09-21T15:16:37.555710Z",
     "shell.execute_reply.started": "2023-09-21T15:16:37.553631Z"
    }
   },
   "source": [
    "### test the pileup theory here ...\n",
    "\n",
    "are snippets returned in the same order they are in the dataframe ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10a8aef-6c11-41ab-8684-029faa6a9b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# just a single pileup to see ...\n",
    "img = plt.imshow(\n",
    "    np.nanmean(fullstack, axis=0),\n",
    "    norm=LogNorm(vmin=1/4,vmax=4),\n",
    "    interpolation=\"none\",\n",
    "    extent=[-_flank//1000, _flank//1000, -_flank//1000, _flank//1000],\n",
    "    cmap='RdBu_r'\n",
    ")\n",
    "plt.colorbar(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29f6693-5162-4e63-8a03-ba29e6e2f28b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_examples = len(clustered_pixels)\n",
    "\n",
    "@interact(i=(0, n_examples-1))\n",
    "def f(i):\n",
    "    _snip = fullstack[i, :, :]\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(ncols=3,figsize=[15,5])\n",
    "    # ax1 ...\n",
    "    ax1.plot(_snip.sum(axis=0))\n",
    "    ax1.set_xlim(0,2*_flank//binsize)\n",
    "    ax1.set_xticks([0,_flank//binsize, 2*_flank//binsize])\n",
    "    ax1.set_xticklabels([-_flank//1000,0,_flank//1000])\n",
    "    # ax2 ...\n",
    "    ax2.plot(_snip.sum(axis=1))\n",
    "    ax2.set_xlim(0,2*_flank//binsize)\n",
    "    ax2.set_xticks([0,_flank//binsize, 2*_flank//binsize])\n",
    "    ax2.set_xticklabels([-_flank//1000,0,_flank//1000])\n",
    "    # ax3 ...\n",
    "    ax3.imshow(\n",
    "        _snip,\n",
    "        norm=LogNorm(vmin=1/4,vmax=4),\n",
    "        interpolation=\"none\",\n",
    "        extent=[-_flank//1000, _flank//1000, -_flank//1000, _flank//1000],\n",
    "        cmap='coolwarm'\n",
    "    )\n",
    "    ax3.axvline(0, c='g', ls=':')\n",
    "    ax3.axhline(0, c='g', ls=':')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a9063b-eda4-4783-9905-843316584e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# f, axs = plt.subplots(nrows=1,ncols=len(stacks)+1,figsize=(20,5),width_ratios=[1]*len(stacks)+[0.05])\n",
    "nquant = 7\n",
    "dist_bins = np.linspace(0,1,nquant+1)\n",
    "f, axs = plt.subplots(\n",
    "    nrows=1,\n",
    "    ncols=nquant+1,\n",
    "    figsize=(3.2*nquant,3),\n",
    "    width_ratios=[1]*nquant+[0.05]\n",
    ")\n",
    "snp_grp = clustered_pixels.groupby(\n",
    "    pd.qcut(clustered_pixels[\"dist\"], dist_bins)\n",
    ")\n",
    "\n",
    "for i, (ax, (_q, _mtx)) in enumerate(zip(axs, snp_grp.groups.items())):\n",
    "    _ccc = ax.imshow(\n",
    "        np.nanmean(fullstack[_mtx,:,:], axis=0),\n",
    "        cmap='RdBu_r',\n",
    "        norm=LogNorm(vmin=1/4,vmax=4)\n",
    "    )\n",
    "    ticks_pixels = np.linspace(0, _flank*2//binsize, 5)\n",
    "    ticks_kbp = ((ticks_pixels-ticks_pixels[-1]/2)*binsize//1000).astype(int)\n",
    "    ax.set_title(f\"{_q}\")\n",
    "    ax.set_xticks(ticks_pixels, ticks_kbp)\n",
    "    ax.set_yticks(ticks_pixels, ticks_kbp)\n",
    "    ax.set_xlabel('relative position, kbp')\n",
    "    if i<1:\n",
    "        ax.set_ylabel('relative position, kbp')\n",
    "    if i==nquant-1:\n",
    "        plt.colorbar(_ccc, label = 'obs/exp', cax=axs[i+1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b075f4b2-bb3d-48ec-87ea-e0ce40becb3a",
   "metadata": {},
   "source": [
    "## try by pileup by region and by distance at the same time ! ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a67dc9-6abd-490f-87ce-d9bde8b434df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reg_grp = clustered_pixels.groupby( \"region\" )\n",
    "\n",
    "nquant = 7\n",
    "dist_bins = np.linspace(0,1,nquant+1)\n",
    "\n",
    "for reg, gdf in reg_grp:\n",
    "    if not len(gdf):\n",
    "        print(f\"... skipping {reg}, cause empty ...\")\n",
    "        continue\n",
    "    f, axs = plt.subplots(\n",
    "        nrows=1,\n",
    "        ncols=nquant+1,\n",
    "        figsize=(3.2*nquant,3),\n",
    "        width_ratios=[1]*nquant+[0.05]\n",
    "    )\n",
    "    snp_grp = gdf.groupby( pd.qcut(gdf[\"dist\"], dist_bins) )\n",
    "    f.suptitle(f\"{reg}\")\n",
    "\n",
    "    for i, (ax, (_q, _mdf)) in enumerate(zip(axs, snp_grp)):\n",
    "        _ccc = ax.imshow(\n",
    "            np.nanmean(fullstack[_mdf.index,:,:], axis=0),\n",
    "            cmap='RdBu_r',\n",
    "            norm=LogNorm(vmin=1/4,vmax=4)\n",
    "        )\n",
    "        ticks_pixels = np.linspace(0, _flank*2//binsize, 5)\n",
    "        ticks_kbp = ((ticks_pixels-ticks_pixels[-1]/2)*binsize//1000).astype(int)\n",
    "        ax.set_title(f\"{_q}: {len(_mdf)}\")\n",
    "        ax.set_xticks(ticks_pixels, ticks_kbp)\n",
    "        ax.set_yticks(ticks_pixels, ticks_kbp)\n",
    "        ax.set_xlabel('relative position, kbp')\n",
    "        if i<1:\n",
    "            ax.set_ylabel('relative position, kbp')\n",
    "        if i==nquant-1:\n",
    "            plt.colorbar(_ccc, label = 'obs/exp', cax=axs[i+1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53282fc-9034-47b1-a7f7-8ba6454c99b8",
   "metadata": {},
   "source": [
    "### try a few more groupings ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878a2b39-3d6e-4c68-bad9-d41bc8f0c815",
   "metadata": {},
   "outputs": [],
   "source": [
    "gfeat_name = \"cluster_size\"\n",
    "the_grp = clustered_pixels.groupby( gfeat_name )\n",
    "\n",
    "nquant = 5\n",
    "dist_bins = np.linspace(0,1,nquant+1)\n",
    "\n",
    "\n",
    "\n",
    "for _gname, gdf in the_grp:\n",
    "    if len(gdf)>20:\n",
    "        # split by distnace - cause enough data ...\n",
    "        f, axs = plt.subplots(\n",
    "            nrows=1,\n",
    "            ncols=nquant+1,\n",
    "            figsize=(3.2*nquant,3),\n",
    "            width_ratios=[1]*nquant+[0.05]\n",
    "        )\n",
    "        snp_grp = gdf.groupby( pd.qcut(gdf[\"dist\"], dist_bins) )\n",
    "        f.suptitle(f\"{gfeat_name}: {_gname}\")\n",
    "\n",
    "        for i, (ax, (_q, _mdf)) in enumerate(zip(axs, snp_grp)):\n",
    "            _ccc = ax.imshow(\n",
    "                np.nanmean(fullstack[_mdf.index,:,:], axis=0),\n",
    "                cmap='RdBu_r',\n",
    "                norm=LogNorm(vmin=1/4,vmax=4)\n",
    "            )\n",
    "            ticks_pixels = np.linspace(0, _flank*2//binsize, 5)\n",
    "            ticks_kbp = ((ticks_pixels-ticks_pixels[-1]/2)*binsize//1000).astype(int)\n",
    "            ax.set_title(f\"{_q}: {len(_mdf)}\")\n",
    "            ax.set_xticks(ticks_pixels, ticks_kbp)\n",
    "            ax.set_yticks(ticks_pixels, ticks_kbp)\n",
    "            ax.set_xlabel('relative position, kbp')\n",
    "            if i<1:\n",
    "                ax.set_ylabel('relative position, kbp')\n",
    "            if i==nquant-1:\n",
    "                plt.colorbar(_ccc, label = 'obs/exp', cax=axs[i+1])\n",
    "    else:\n",
    "        print(f\"cannot split {_gname} by dist, cause too small ...\")\n",
    "        f, ax = plt.subplots(\n",
    "            nrows=1,\n",
    "            ncols=1,\n",
    "            figsize=(5,5),\n",
    "        )\n",
    "        f.suptitle(f\"{gfeat_name}: {_gname}\")\n",
    "        _ccc = ax.imshow(\n",
    "            np.nanmean(fullstack[gdf.index,:,:], axis=0),\n",
    "            cmap='RdBu_r',\n",
    "            norm=LogNorm(vmin=1/4,vmax=4)\n",
    "        )\n",
    "        ticks_pixels = np.linspace(0, _flank*2//binsize, 5)\n",
    "        ticks_kbp = ((ticks_pixels-ticks_pixels[-1]/2)*binsize//1000).astype(int)\n",
    "        ax.set_title(f\"{_q}: {len(_mdf)}\")\n",
    "        ax.set_xticks(ticks_pixels, ticks_kbp)\n",
    "        ax.set_yticks(ticks_pixels, ticks_kbp)\n",
    "        ax.set_xlabel('relative position, kbp')\n",
    "        ax.set_ylabel('relative position, kbp')\n",
    "        plt.colorbar(_ccc, label = 'obs/exp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97020ef4-e24f-4387-9503-0c11196fcd3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
