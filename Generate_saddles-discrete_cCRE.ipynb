{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saddleplots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## we'll just generate the data here - and them we'll reuse them in a separate notebook for plotting !\n",
    "\n",
    "Welcome to the compartments and saddleplot notebook! \n",
    "\n",
    "This notebook illustrates cooltools functions used for investigating chromosomal compartments, visible as plaid patterns in mammalian interphase contact frequency maps.\n",
    "\n",
    "These plaid patterns reflect tendencies of chromosome regions to make more frequent contacts with regions of the same type: active regions have increased contact frequency with other active regions, and intactive regions tend to contact other inactive regions more frequently. The strength of compartmentalization has been show to vary through the cell cycle, across cell types, and after degredation of components of the cohesin complex. \n",
    "\n",
    "In this notebook we:\n",
    "\n",
    "* obtain compartment profiles using eigendecomposition\n",
    "* calculate and visualize strength of compartmentalization using saddleplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import standard python libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import os, subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import python package for working with cooler files and tools for analysis\n",
    "import cooler\n",
    "import cooltools.lib.plotting\n",
    "import bioframe\n",
    "import multiprocess as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# from saddle import saddleplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download test data\n",
    "# this file is 145 Mb, and may take a few seconds to download\n",
    "import bbi\n",
    "import cooltools\n",
    "import bioframe\n",
    "from matplotlib.colors import LogNorm\n",
    "from helper_func import saddleplot\n",
    "from data_catalog import bws, bws_vlim, telo_dict\n",
    "\n",
    "import saddle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from tqdm.notebook import trange, tqdm\n",
    "import warnings\n",
    "import seaborn as sns\n",
    "\n",
    "from mpire import WorkerPool\n",
    "import warnings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating per-chromosome compartmentalization\n",
    "\n",
    "We first load the Hi-C data at 100 kbp resolution. \n",
    "\n",
    "Note that the current implementation of eigendecomposition in cooltools assumes that individual regions can be held in memory-- for hg38 at 100kb this is either a 2422x2422 matrix for chr2, or a 3255x3255 matrix for the full cooler here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define genomic view that will be used to call dots and pre-compute expected\n",
    "\n",
    "# Use bioframe to fetch the genomic features from the UCSC.\n",
    "hg38_chromsizes = bioframe.fetch_chromsizes('hg38')\n",
    "hg38_cens = bioframe.fetch_centromeres('hg38')\n",
    "hg38_arms_full = bioframe.make_chromarms(hg38_chromsizes, hg38_cens)\n",
    "# # remove \"bad\" chromosomes and near-empty arms ...\n",
    "included_arms = hg38_arms_full[\"name\"].to_list()[:44] # all autosomal ones ...\n",
    "hg38_arms = hg38_arms_full[hg38_arms_full[\"name\"].isin(included_arms)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pre-load coolers and pre-calculate expected ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cooler files that we'll work on :\n",
    "binsize10 = 10_000\n",
    "telo_clrs10 = { _k: cooler.Cooler(f\"{_path}::/resolutions/{binsize10}\") for _k, _path in telo_dict.items() }\n",
    "binsize25 = 25_000\n",
    "telo_clrs25 = { _k: cooler.Cooler(f\"{_path}::/resolutions/{binsize25}\") for _k, _path in telo_dict.items() }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def _job(packed_data, sample):\n",
    "    # packed data -> exp_kwargs and a dict with coolers for each sample\n",
    "    exp_kwargs, clr_dict = packed_data\n",
    "    _clr = clr_dict[sample]\n",
    "    # in order to use spawn/forkserver we have to import for worker\n",
    "    from cooltools import expected_cis\n",
    "    _exp = expected_cis( _clr, **exp_kwargs)\n",
    "    return (sample, _exp)\n",
    "\n",
    "# define expected parameters in the form of kwargs-dict:\n",
    "exp_kwargs = dict(\n",
    "    view_df=hg38_arms,\n",
    "    intra_only=False,\n",
    "    nproc=12\n",
    ")\n",
    "\n",
    "# have to use daemon=False, because _job is multiprocessing-based already ...\n",
    "with WorkerPool(\n",
    "    n_jobs=8,\n",
    "    daemon=False,\n",
    "    shared_objects=( exp_kwargs, telo_clrs10 ),\n",
    "    start_method=\"forkserver\",  # little faster than spawn, fork is the fastest\n",
    "    use_dill=True,\n",
    ") as wpool:\n",
    "    results = wpool.map(_job, telo_clrs10, progress_bar=True)\n",
    "\n",
    "# sort out the results ...\n",
    "telo_exps_cis = {sample: _exp for sample, _exp in results}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### trans-expected second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def _job(packed_data, sample):\n",
    "    # unpack data\n",
    "    clr_dict, = packed_data\n",
    "    exp_kwargs = dict(chunksize=1000000, nproc=12)\n",
    "    from cooltools import expected_trans\n",
    "    _clr = clr_dict[sample]\n",
    "    _exp = expected_trans( _clr, **exp_kwargs).set_index([\"region1\", \"region2\"]).sort_index()\n",
    "    return (sample, _exp)\n",
    "\n",
    "# have to use daemon=False, because _job is multiprocessing-based already ...\n",
    "with WorkerPool(\n",
    "    n_jobs=8,\n",
    "    daemon=False,\n",
    "    shared_objects=(telo_clrs25, ),\n",
    "    start_method=\"forkserver\",\n",
    "    use_dill=True,\n",
    ") as wpool:\n",
    "    results = wpool.map(_job, telo_clrs25, progress_bar=True)\n",
    "\n",
    "# sort out the results ...\n",
    "telo_exps_trans = {sample: _exp for sample, _exp in results}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load partition of the genome into clusters ...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saddleplots\n",
    "\n",
    "A common way to visualize preferences captured by the eigenvector is by using saddleplots.\n",
    "\n",
    "To generate a saddleplot, we first use the eigenvector to stratify genomic regions into groups with similar values of the eigenvector. These groups are then averaged over to create the saddleplot.\n",
    "This process is called \"digitizing\".\n",
    "\n",
    "Cooltools will operate with `digitized` bedgraph-like track with four columns. The fourth, or value, column is a categorical, as shown above for the first three bins. Categories have the following encoding:\n",
    "\n",
    "    - `1..n` <-> values assigned to bins defined by vrange or qrange\n",
    "    - `0` <-> left outlier values\n",
    "    - `n+1` <-> right outlier values\n",
    "    - `-1` <-> missing data (NaNs)\n",
    "    \n",
    "Track values can either be digitized by numeric values, by passing `vrange`, or by quantiles, by passing `qrange`, as above.\n",
    "\n",
    "To create saddles in cis with `saddle`, cooltools requires: a cooler, a table with expected as function of distance, and parameters for digitizing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telo_trans_filt_exps = {}\n",
    "for _k, _clr in tqdm(telo_clrs25.items()):\n",
    "    _df = telo_exps_trans[_k].reset_index()\n",
    "    m2 = _df[\"region2\"].isin([\"chrX\",\"chrY\",\"chrM\"])\n",
    "    m1 = _df[\"region1\"].isin([\"chrX\",\"chrY\",\"chrM\"])\n",
    "    telo_trans_filt_exps[_k] = _df[~(m1 | m2)]\n",
    "\n",
    "# a view without M,X and Y chromosomes ...\n",
    "sub_chrom_view = bioframe.make_viewframe(hg38_chromsizes)\n",
    "bad_chroms = [\"chrX\",\"chrY\",\"chrM\"]\n",
    "sub_chrom_view = sub_chrom_view[~sub_chrom_view[\"name\"].isin(bad_chroms)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's generate CRE track with Allana-style bin assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! scp newhpc:/home/allana.schooley-umw/as38w/Ranger/cres/all_cres_encodestyle_pMunion_NEWatac_k2785_rest95.bed ./bigbed/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! scp newhpc:/home/allana.schooley-umw/as38w/Ranger/cres/all_cres_encodestyle_pMunion_95atac_95rest.bed ./bigbed/\n",
    "# ! scp newhpc:/home/allana.schooley-umw/as38w/Ranger/cres/all_cres_encodestyle_pMunion_90atac_90rest.bed ./bigbed/\n",
    "# ! scp newhpc:/home/allana.schooley-umw/as38w/Ranger/cres/all_cres_encodestyle_pMunion_70atac_70rest.bed ./bigbed/\n",
    "# ! scp newhpc:/home/allana.schooley-umw/as38w/Ranger/cres/all_cres_encodestyle_pMunion_70atac_95rest.bed ./bigbed/\n",
    "# ! scp newhpc:/home/allana.schooley-umw/as38w/Ranger/cres/all_cres_encodestyle_pMunion_70atac_90rest.bed ./bigbed/\n",
    "# ! scp newhpc:/home/allana.schooley-umw/as38w/Ranger/cres/all_cres_encodestyle_pMunion_75atac_95rest.bed ./bigbed/\n",
    "# ! scp newhpc:/home/allana.schooley-umw/as38w/Ranger/cres/all_cres_encodestyle_pMunion_75atac_90rest.bed ./bigbed/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# old way of stitching cCREs into a binned_CRE list ...\n",
    "\n",
    "# # different CRE will be assigned categroies corresponding to their importance\n",
    "# cre_fnames = {\n",
    "#     \"PLS\" : \"bigbed/PLS_ccres.bed\",\n",
    "#     \"pELS\" : \"bigbed/pELS_ccres.bed\",\n",
    "#     \"dELS\" : \"bigbed/dELS_ccres.bed\",\n",
    "#     \"openK4\" : \"bigbed/openK4_ccres.bed\",\n",
    "#     \"ctcf\" : \"bigbed/ctcf_ccres.bed\",\n",
    "#     \"justOpen\" : \"bigbed/justOpen_ccres.bed\",\n",
    "# }\n",
    "# ticklabels = [\n",
    "#     \"PLS\",\n",
    "#     \"pELS\",\n",
    "#     \"dELS\",\n",
    "#     \"opK4\",\n",
    "#     \"CTCF\",\n",
    "#     \"open\",\n",
    "#     \"none\",\n",
    "# ]\n",
    "# categroies are going to be 1,2,3,4,5,6\n",
    "# 0 - everything else ...\n",
    "# cres = []\n",
    "# for i, (k,fname) in enumerate(cre_fnames.items()):\n",
    "#     #\n",
    "#     df = pd.read_table(fname, sep=\"\\t\")[[\"chrom\",\"start\",\"end\"]]\n",
    "#     df[\"status\"] = len(cre_fnames) - i\n",
    "#     cres.append(df)\n",
    "\n",
    "# cres_df = pd.concat(cres, ignore_index=True)\n",
    "# cres_df = bioframe.sort_bedframe(cres_df, view_df=hg38_arms)\n",
    "# # now assign CREs to bins ...\n",
    "# display(clr_bins.head(2))\n",
    "# display(clr_bins.tail(2))\n",
    "# # now annotate bins with the anchors ...\n",
    "# _bin_cres = bioframe.overlap(clr_bins, cres_df)\n",
    "# # _ggg[\"status\"]\n",
    "# _bin_cres = _bin_cres.drop(columns=[\"chrom_\",\"start_\",\"end_\"])\n",
    "# _bin_cres[\"status_\"] = _bin_cres[\"status_\"].fillna(0).astype(int)\n",
    "# # ...\n",
    "# binned_cres = _bin_cres.groupby([\"chrom\",\"start\",\"end\"], observed=True)[\"status_\"].max().reset_index()\n",
    "# binned_cres[\"status_\"] = binned_cres[\"status_\"].astype(\"category\")\n",
    "# binned_cres = binned_cres.astype({\"chrom\":str})\n",
    "# # ...\n",
    "# binned_cres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate some binsized bins\n",
    "clr_bins10 = cooler.binnify(hg38_chromsizes, binsize10)\n",
    "clr_bins10 = clr_bins10[~clr_bins10[\"chrom\"].isin([\"chrX\",\"chrY\",\"chrM\"])]\n",
    "# now annotate those bins with useful info - e.g. ID anchors ...\n",
    "display(clr_bins10.head(2))\n",
    "display(clr_bins10.tail(2))\n",
    "\n",
    "# generate some binsized bins\n",
    "clr_bins25 = cooler.binnify(hg38_chromsizes, binsize25)\n",
    "clr_bins25 = clr_bins25[~clr_bins25[\"chrom\"].isin([\"chrX\",\"chrY\",\"chrM\"])]\n",
    "# now annotate those bins with useful info - e.g. ID anchors ...\n",
    "display(clr_bins25.head(2))\n",
    "display(clr_bins25.tail(2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cre_to_code = {\n",
    "    'pls':6,\n",
    "    'pels':5,\n",
    "    'nels':5,\n",
    "    'dels':4,\n",
    "    'openk4':3,\n",
    "    'ctcf':2,\n",
    "    'justopen':1,\n",
    "}\n",
    "\n",
    "cre_fnames = {\n",
    "    \"atac95_rest95\" : \"./bigbed/all_cres_encodestyle_pMunion_95atac_95rest.bed\",  # control sorta, like before\n",
    "    \"atac90_rest90\" : \"./bigbed/all_cres_encodestyle_pMunion_90atac_90rest.bed\",\n",
    "    \"atac70_rest70\" : \"./bigbed/all_cres_encodestyle_pMunion_70atac_70rest.bed\",\n",
    "    \"atac70_rest95\" : \"./bigbed/all_cres_encodestyle_pMunion_70atac_95rest.bed\",\n",
    "    \"atac70_rest90\" : \"./bigbed/all_cres_encodestyle_pMunion_70atac_90rest.bed\",\n",
    "    \"atac75_rest95\" : \"./bigbed/all_cres_encodestyle_pMunion_75atac_95rest.bed\",\n",
    "    \"atac75_rest90\" : \"./bigbed/all_cres_encodestyle_pMunion_75atac_90rest.bed\",\n",
    "    \"allana_latest\" : \"./bigbed/all_cres_encodestyle_pMunion_NEWatac_k2785_rest95.bed\",\n",
    "}\n",
    "\n",
    "binned_cre10_dfs = {}\n",
    "binned_cre25_dfs = {}\n",
    "for name, cre_fname in cre_fnames.items():\n",
    "    df = pd.read_table(cre_fname, usecols=['chrom', 'start', 'end', 'name'])\n",
    "    print(f\"read {name=} with {len(df)} items ...\")\n",
    "    # # rename nels -> pels\n",
    "    # df = df.replace({'name': {\"nels\": \"pels\"}})\n",
    "    # sort by coordinate\n",
    "    df = bioframe.sort_bedframe(df, view_df=hg38_arms)\n",
    "    df[\"status\"] = df[\"name\"].map(cre_to_code)\n",
    "\n",
    "    # now annotate bins with the anchors @10kb  ...\n",
    "    _bin_cres = bioframe.overlap(clr_bins10, df)\n",
    "    _bin_cres = _bin_cres.drop(columns=[\"chrom_\",\"start_\",\"end_\"])\n",
    "    _bin_cres[\"status_\"] = _bin_cres[\"status_\"].fillna(0).astype(int)\n",
    "    # assign cCREs per bin according to the hierarchy ...\n",
    "    binned_cres = _bin_cres.groupby([\"chrom\",\"start\",\"end\"], observed=True)[\"status_\"].max().reset_index()\n",
    "    binned_cres[\"status_\"] = binned_cres[\"status_\"].astype(\"category\")\n",
    "    # ...\n",
    "    binned_cre10_dfs[name] = binned_cres.astype({ \"chrom\" : str })\n",
    "\n",
    "    # now annotate bins with the anchors @10kb  ...\n",
    "    _bin_cres = bioframe.overlap(clr_bins25, df)\n",
    "    _bin_cres = _bin_cres.drop(columns=[\"chrom_\",\"start_\",\"end_\"])\n",
    "    _bin_cres[\"status_\"] = _bin_cres[\"status_\"].fillna(0).astype(int)\n",
    "    # assign cCREs per bin according to the hierarchy ...\n",
    "    binned_cres = _bin_cres.groupby([\"chrom\",\"start\",\"end\"], observed=True)[\"status_\"].max().reset_index()\n",
    "    binned_cres[\"status_\"] = binned_cres[\"status_\"].astype(\"category\")\n",
    "    # ...\n",
    "    binned_cre25_dfs[name] = binned_cres.astype({ \"chrom\" : str })\n",
    "\n",
    "\n",
    "\n",
    "ticklabels = [\n",
    "    \"PLS\",\n",
    "    \"pELS\",\n",
    "    \"dELS\",\n",
    "    \"opK4\",\n",
    "    \"CTCF\",\n",
    "    \"open\",\n",
    "    \"none\",\n",
    "]\n",
    "# categroies are going to be 1,2,3,4,5,6\n",
    "# 0 - everything else ...\n",
    "\n",
    "\n",
    "_working_cres10 = binned_cre10_dfs['allana_latest']\n",
    "_working_cres25 = binned_cre25_dfs['allana_latest']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_anchor_fnames = {\n",
    "    \"mega_2X_enrichment\": \"ID_anchors/mega_2X_enrichment.fourth_mega.max_size.bed\",\n",
    "    \"5hr_2X_enrichment_old\": \"ID_anchors/5hr_2X_enrichment.second_bulk.max_size.bed\",\n",
    "    \"5hr_2X_enrichment\": \"ID_anchors/5hr_2X_enrichment.pixel_derived.bed\",\n",
    "    \"5hr_2X_enrichment_nosing\": \"ID_anchors/5hr_2X_enrichment.pixel_derived.no_singletons.bed\",\n",
    "    \"5hr_notinCyto_2X_enrichment_signal\": \"ID_anchors/p5notin_pCyto_anchors_2X_enrichment.pixel_derived.signal_peaks.bed\",\n",
    "    \"5hr_2X_enrichment_signal\": \"ID_anchors/5hr_2X_enrichment.pixel_derived.signal_peaks.bed\",\n",
    "    \"10hr_2X_enrichment_signal\": \"ID_anchors/10hrs_2X_enrichment.pixel_derived.signal_peaks.bed\",\n",
    "    \"N93p5_2X_enrichment_signal\": \"ID_anchors/N93p5_2X_enrichment.pixel_derived.signal_peaks.bed\",\n",
    "    \"pCyto_2X_enrichment_signal\": \"ID_anchors/pCyto_2X_enrichment.pixel_derived.signal_peaks.bed\",\n",
    "    \"mCyto_2X_enrichment_signal\": \"ID_anchors/mCyto_2X_enrichment.pixel_derived.signal_peaks.bed\",\n",
    "    \"mega_3X_enrichment\": \"ID_anchors/mega_3X_enrichment.fifth_mega3x.max_size.bed\",\n",
    "    \"MEGA_2X_enrichment\": \"ID_anchors/MEGAp5_2X_enrichment.pixel_derived.signal_peaks.bed\",\n",
    "    \"MEGA_weaker_2X_enrichment\": \"ID_anchors/MEGA_plus_weak_anchors_2X_enrichment.pixel_derived.signal_peaks.bed\",\n",
    "    \"MEGAN93_2X_enrichment\": \"ID_anchors/MEGAN93p5_2X_enrichment.pixel_derived.signal_peaks.bed\",\n",
    "    \"MEGAminus_2X_enrichment\": \"ID_anchors/MEGA_minus_ctrl_2X_enrichment.pixel_derived.signal_peaks.bed\",\n",
    "    \"cyto_2x_enrichment\": \"ID_anchors/cyto_2x_enrichment.third_mCyto.max_size.bed\",\n",
    "}\n",
    "\n",
    "id_anchors_dict = {}\n",
    "for id_name, fname in id_anchor_fnames.items():\n",
    "    id_anchors_dict[id_name] = pd.read_csv(fname, sep=\"\\t\")\n",
    "    # ...\n",
    "    print(f\"loaded {len(id_anchors_dict[id_name]):5d} ID anchors {id_name:>20} in BED format ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check enrichemnts of Nezar's IPGs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nezar_df = bioframe.read_table(\"GSM7990272_DLD1.360.NT.50000.E1-E128.comp_10.kmeans9_5.bed\", schema=\"bed9\")\n",
    "\n",
    "# Now make saddle-compatible track - i.e. digitized into 0,1,2,3,...categories\n",
    "_comp_dict = {\n",
    "    'A1':2,\n",
    "    'A2':3,\n",
    "    'V+VI':4,\n",
    "    'B2/B3':5,\n",
    "    'B4':6,\n",
    "}\n",
    "\n",
    "k = \"name\"\n",
    "_track = nezar_df[[\"chrom\",\"start\",\"end\",k]].replace({k: _comp_dict})#.astype({k:\"category\"})\n",
    "_track[k].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate some binsized bins\n",
    "clr_bins50 = cooler.binnify(hg38_chromsizes, 50_000)\n",
    "clr_bins50 = clr_bins50[~clr_bins50[\"chrom\"].isin([\"chrX\",\"chrY\",\"chrM\"])]\n",
    "\n",
    "# now annotate bins with the anchors @10kb  ...\n",
    "_bin_assigned = bioframe.overlap(clr_bins50, _track)\n",
    "_bin_assigned = _bin_assigned.drop(columns=[\"chrom_\",\"start_\",\"end_\"])\n",
    "_bin_assigned[\"name_\"] = _bin_assigned[\"name_\"].fillna(0).astype(int)\n",
    "\n",
    "_cats = pd.CategoricalDtype(categories=list(range(7)), ordered=True)\n",
    "\n",
    "_bin_assigned[\"name_\"] = _bin_assigned[\"name_\"].astype(_cats)\n",
    "# ...\n",
    "_track_bins = _bin_assigned.astype({ \"chrom\" : str })\n",
    "_track_bins = _track_bins.rename(columns={\"name_\":\"name\"})\n",
    "_track_bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # indices of compartments back to their names ...\n",
    "# name_to_descr = dict((i, name) for name, i in _comp_dict.items())\n",
    "\n",
    "_chosen_idname = \"5hr_2X_enrichment_signal\"\n",
    "# _chosen_idname = \"MEGAminus_2X_enrichment\"\n",
    "# _chosen_idname = \"MEGA_2X_enrichment\"\n",
    "# _chosen_idname = \"pCyto_2X_enrichment_signal\"\n",
    "_anchors = id_anchors_dict[_chosen_idname]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_track_bins_id = _track_bins.copy()\n",
    "# ...\n",
    "_id_bins = bioframe.overlap( _track_bins, _anchors, return_index=True).dropna()[\"index\"]\n",
    "# ...\n",
    "_track_bins_id.loc[_id_bins, \"name\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cre_to_code = {\n",
    "    'pls':6,\n",
    "    'pels':5,\n",
    "    'nels':5,\n",
    "    'dels':4,\n",
    "    'openk4':3,\n",
    "    'ctcf':2,\n",
    "    'justopen':1,\n",
    "}\n",
    "\n",
    "_cats = pd.CategoricalDtype(categories=list(range(6+1)), ordered=True)\n",
    "\n",
    "cre_fnames = {\n",
    "    # \"atac95_rest95\" : \"./bigbed/all_cres_encodestyle_pMunion_95atac_95rest.bed\",  # control sorta, like before\n",
    "    # \"atac90_rest90\" : \"./bigbed/all_cres_encodestyle_pMunion_90atac_90rest.bed\",\n",
    "    # \"atac70_rest70\" : \"./bigbed/all_cres_encodestyle_pMunion_70atac_70rest.bed\",\n",
    "    # \"atac70_rest95\" : \"./bigbed/all_cres_encodestyle_pMunion_70atac_95rest.bed\",\n",
    "    # \"atac70_rest90\" : \"./bigbed/all_cres_encodestyle_pMunion_70atac_90rest.bed\",\n",
    "    # \"atac75_rest95\" : \"./bigbed/all_cres_encodestyle_pMunion_75atac_95rest.bed\",\n",
    "    # \"atac75_rest90\" : \"./bigbed/all_cres_encodestyle_pMunion_75atac_90rest.bed\",\n",
    "    \"allana_latest\" : \"./bigbed/all_cres_encodestyle_pMunion_NEWatac_k2785_rest95.bed\",\n",
    "}\n",
    "\n",
    "_status_query = \"(status_ == 0)|(status_ == 5)|(status_ == 4)|(status_ == 6)|(status_ == 7)\"\n",
    "\n",
    "binned_cre10_dfs = {}\n",
    "binned_cre25_dfs = {}\n",
    "for name, cre_fname in cre_fnames.items():\n",
    "    df = pd.read_table(cre_fname, usecols=['chrom', 'start', 'end', 'name'])\n",
    "    print(f\"read {name=} with {len(df)} items ...\")\n",
    "    # # rename nels -> pels\n",
    "    # df = df.replace({'name': {\"nels\": \"pels\"}})\n",
    "    # sort by coordinate\n",
    "    df = bioframe.sort_bedframe(df, view_df=hg38_arms)\n",
    "    df[\"status\"] = df[\"name\"].map(cre_to_code)\n",
    "\n",
    "    # now annotate bins with the anchors @10kb  ...\n",
    "    _bin_cres = bioframe.overlap(clr_bins10, df)\n",
    "    _bin_cres = _bin_cres.drop(columns=[\"chrom_\",\"start_\",\"end_\"])\n",
    "    _bin_cres[\"status_\"] = _bin_cres[\"status_\"].fillna(0).astype(int)\n",
    "    # assign cCREs per bin according to the hierarchy ...\n",
    "    binned_cres = _bin_cres.groupby([\"chrom\",\"start\",\"end\"], observed=True)[\"status_\"].max().reset_index()\n",
    "    binned_cres[\"status_\"] = binned_cres[\"status_\"].astype(_cats)\n",
    "    # value_counts of annotated bins before gene assignment ...\n",
    "    print(\"@10kb\\n\")\n",
    "    display(binned_cres[\"status_\"].value_counts().sort_index())\n",
    "    print(\"\\n\")\n",
    "    #\n",
    "    binned_cre10_dfs[name] = binned_cres.astype({ \"chrom\" : str })\n",
    "\n",
    "    # now annotate bins with the anchors @25kb  ...\n",
    "    _bin_cres = bioframe.overlap(clr_bins25, df)\n",
    "    _bin_cres = _bin_cres.drop(columns=[\"chrom_\",\"start_\",\"end_\"])\n",
    "    _bin_cres[\"status_\"] = _bin_cres[\"status_\"].fillna(0).astype(int)\n",
    "    # assign cCREs per bin according to the hierarchy ...\n",
    "    binned_cres = _bin_cres.groupby([\"chrom\",\"start\",\"end\"], observed=True)[\"status_\"].max().reset_index()\n",
    "    binned_cres[\"status_\"] = binned_cres[\"status_\"].astype(_cats)\n",
    "    # value_counts of annotated bins before gene assignment ...\n",
    "    print(\"@25kb\\n\")\n",
    "    display(binned_cres[\"status_\"].value_counts().sort_index())\n",
    "    print(\"\\n\")\n",
    "    # ...\n",
    "    binned_cre25_dfs[name] = binned_cres.astype({ \"chrom\" : str })\n",
    "\n",
    "\n",
    "\n",
    "ticklabels = [\n",
    "    \"PLS\",\n",
    "    \"pELS\",\n",
    "    \"dELS\",\n",
    "    \"opK4\",\n",
    "    \"CTCF\",\n",
    "    \"open\",\n",
    "    \"none\",\n",
    "]\n",
    "# # categroies are going to be 1,2,3,4,5,6\n",
    "# # 0 - everything else ...\n",
    "\n",
    "_working_cres10 = binned_cre10_dfs[\"allana_latest\"]\n",
    "_working_cres25 = binned_cre25_dfs[\"allana_latest\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"name\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "26855+2008"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! head ./bigbed/all_cres_encodestyle_pMunion_NEWatac_k2785_rest95.bed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ooo = []\n",
    "for k in range(7):\n",
    "    _xxx = bioframe.overlap(_working_cres10, _track_bins_id).dropna().query(f\"name_ == {k}\")[\"status_\"].value_counts().sort_index()\n",
    "    _xxx.name = k\n",
    "    ooo.append(_xxx)\n",
    "_mat1 = pd.concat(ooo, axis=1)\n",
    "print(_mat1)\n",
    "_mmm1 = ((_mat1/_mat1.sum(axis=0)).T / (_mat1.sum(axis=1)/len(_working_cres10))).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ooo = []\n",
    "for k in range(7):\n",
    "    _xxx = bioframe.overlap(_working_cres10, _track_bins).dropna().query(f\"name_ == {k}\")[\"status_\"].value_counts().sort_index()\n",
    "    _xxx.name = k\n",
    "    ooo.append(_xxx)\n",
    "_mat2 = pd.concat(ooo, axis=1)\n",
    "print(_mat2)\n",
    "_mmm2 = ((_mat2/_mat2.sum(axis=0)).T / (_mat2.sum(axis=1)/len(_working_cres10))).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,axs  = plt.subplots(ncols=2,sharey=True)\n",
    "\n",
    "\n",
    "\n",
    "axs[0].imshow(\n",
    "    _mmm1,\n",
    "    vmin=0,\n",
    "    vmax=5,\n",
    ")\n",
    "\n",
    "\n",
    "axs[1].imshow(\n",
    "    _mmm2,\n",
    "    vmin=0,\n",
    "    vmax=5,\n",
    ")\n",
    "\n",
    "for i,ax in enumerate(axs):\n",
    "    ax.set_xticks(np.arange(7))\n",
    "    ax.set_xticklabels([\"none\",\"id\"]+list(_comp_dict))\n",
    "    if i ==0 :\n",
    "        ax.set_yticks(np.arange(len(ticklabels)))\n",
    "        ax.set_yticklabels(ticklabels[::-1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _job(packed_data, sample):\n",
    "    clr_dict, exp_dict, _atrack, view_df = packed_data\n",
    "    _clr = clr_dict[sample]\n",
    "    _exp = exp_dict[sample]\n",
    "    from cooltools.api.saddle import saddle_stack\n",
    "    _sum, _count = saddle_stack(\n",
    "        _clr,\n",
    "        _exp,\n",
    "        _atrack,\n",
    "        'cis',\n",
    "        n_bins=None,\n",
    "        drop_track_na=True,\n",
    "        view_df=view_df\n",
    "    )\n",
    "    return sample, _sum, _count\n",
    "\n",
    "# have to use daemon=False, because _job is multiprocessing-based already ...\n",
    "with WorkerPool(\n",
    "    n_jobs=16,\n",
    "    daemon=True,\n",
    "    shared_objects=( telo_clrs10, telo_exps_cis, _working_cres10, hg38_arms ),\n",
    "    start_method=\"fork\",  # little faster than spawn, fork is the fastest\n",
    "    use_dill=True,\n",
    ") as wpool:\n",
    "    results = wpool.map(_job, telo_clrs10, progress_bar=True)\n",
    "\n",
    "# sort out the results ...\n",
    "interaction_sums = {}\n",
    "interaction_counts = {}\n",
    "for sample, _sum, _counts in results:\n",
    "    interaction_sums[sample] = _sum\n",
    "    interaction_counts[sample] = _counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trans saddles here yo !\n",
    "def _job_trans(packed_data, sample):\n",
    "    clr_dict, exp_dict, _atrack, view_df = packed_data\n",
    "    _clr = clr_dict[sample]\n",
    "    _exp = exp_dict[sample]\n",
    "    from cooltools.api.saddle import saddle_stack\n",
    "    _sum, _count = saddle_stack(\n",
    "        _clr,\n",
    "        _exp,\n",
    "        _atrack,\n",
    "        'trans',\n",
    "        n_bins=None,\n",
    "        drop_track_na=True,\n",
    "        view_df=view_df,\n",
    "    )\n",
    "    return sample, _sum, _count\n",
    "\n",
    "# have to use daemon=False, because _job_trans is multiprocessing-based already ...\n",
    "with WorkerPool(\n",
    "    n_jobs=16,\n",
    "    daemon=True,\n",
    "    shared_objects=( telo_clrs25, telo_trans_filt_exps, _working_cres25, sub_chrom_view ),\n",
    "    start_method=\"fork\",  # little faster than spawn, fork is the fastest\n",
    "    use_dill=True,\n",
    ") as wpool:\n",
    "    results = wpool.map(_job_trans, telo_clrs25, progress_bar=True)\n",
    "\n",
    "# sort out the results ...\n",
    "interaction_sums_trans = {}\n",
    "interaction_counts_trans = {}\n",
    "for sample, _sum, _counts in results:\n",
    "    interaction_sums_trans[sample] = _sum\n",
    "    interaction_counts_trans[sample] = _counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's save this results using HDF5 for conveniece and to practice ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # we've got 4 dictionaries to store along with some metadata ...\n",
    "# interaction_sums[sample] = _sum\n",
    "# interaction_counts[sample] = _counts\n",
    "# interaction_sums_trans[sample] = _sum\n",
    "# interaction_counts_trans[sample] = _counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(\"saddles_cre_by_distance_Allana_latest_cre.hdf5\", 'x') as f:\n",
    "    # add metadata just in case\n",
    "    f.attrs[\"cis_binsize\"] = binsize10\n",
    "    f.attrs[\"trans_binsize\"] = binsize25\n",
    "    f.attrs[\"cre_fname\"] = \"./bigbed/all_cres_encodestyle_pMunion_95atac_95rest.bed\"\n",
    "    # CIS ...\n",
    "    # interaction_sums ...\n",
    "    _sums_grp = f.create_group(\"sums\")\n",
    "    # create subgroups per sample\n",
    "    for _sample, _arr in interaction_sums.items():\n",
    "        _sums_grp.create_dataset(_sample, data=_arr)\n",
    "    # interaction_counts ...\n",
    "    _sums_grp = f.create_group(\"counts\")\n",
    "    # create subgroups per sample\n",
    "    for _sample, _arr in interaction_counts.items():\n",
    "        _sums_grp.create_dataset(_sample, data=_arr)\n",
    "    # TRANS ...\n",
    "    # interaction_sums ...\n",
    "    _sums_grp = f.create_group(\"sums_trans\")\n",
    "    # create subgroups per sample\n",
    "    for _sample, _arr in interaction_sums_trans.items():\n",
    "        _sums_grp.create_dataset(_sample, data=_arr)\n",
    "    # interaction_counts ...\n",
    "    _sums_grp = f.create_group(\"counts_trans\")\n",
    "    # create subgroups per sample\n",
    "    for _sample, _arr in interaction_counts_trans.items():\n",
    "        _sums_grp.create_dataset(_sample, data=_arr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls -lah saddles_cre_by_distance.hdf5\n",
    "# ! rm saddles_cre_by_distance.hdf5\n",
    "! ls -lah saddles_cre_by_distance_Allana_latest_cre.hdf5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fr = h5py.File(\"saddles_cre_by_distance.hdf5\", 'r')\n",
    "fr = h5py.File(\"saddles_cre_by_distance_Allana_latest_cre.hdf5\", 'r')\n",
    "\n",
    "# fr.items()\n",
    "def print_attrs(name, obj):\n",
    "    # Create indent\n",
    "    shift = name.count('/') * '    '\n",
    "    item_name = name.split(\"/\")[-1]\n",
    "    print(shift + item_name)\n",
    "    try:\n",
    "        for key, val in obj.attrs.items():\n",
    "            print(shift + '    ' + f\"{key}: {val}\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "# f = h5py.File('foo.hdf5','r')\n",
    "fr.visititems(print_attrs)\n",
    "\n",
    "# check general metadata ...\n",
    "dict(fr.attrs)\n",
    "\n",
    "fr.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_samples_m = [\n",
    "    \"mMito\",\n",
    "    \"mTelo\",\n",
    "    \"mCyto\",\n",
    "    \"m5hR1R2\",\n",
    "    \"m10hR1R2\",\n",
    "]\n",
    "sub_samples_p = [\n",
    "    \"pMito\",\n",
    "    \"pTelo\",\n",
    "    \"pCyto\",\n",
    "    \"p5hR1R2\",\n",
    "    \"p10hR1R2\",\n",
    "]\n",
    "\n",
    "# introduce distance ranges\n",
    "# 0-1mb: 0:21 bins\n",
    "# 1-7Mb: 21:141 bins\n",
    "# 7-50Mb: 141:1001 bins\n",
    "distances = {\n",
    "    \"short:<1MB\": slice(0,int(1_000_000/binsize10)+1),\n",
    "    \"mid:1MB-7Mb\": slice(int(1_000_000/binsize10),int(7_000_000/binsize10)+1),\n",
    "    \"long7Mb-50Mb\": slice(int(7_000_000/binsize10),int(50_000_000/binsize10)+1),\n",
    "    \"trans\": slice(None),\n",
    "}\n",
    "\n",
    "# introduce distance ranges\n",
    "# 0-0.5mb\n",
    "# 0.5-10Mb\n",
    "# 10-500Mb\n",
    "distances = {\n",
    "    \"short:<0.5MB\": slice(0,int(500_000/binsize10)+1),\n",
    "    \"mid:0.5MB-10Mb\": slice(int(500_000/binsize10),int(10_000_000/binsize10)+1),\n",
    "    \"long10Mb-100Mb\": slice(int(10_000_000/binsize10),int(500_000_000/binsize10)+1),\n",
    "    \"trans\": slice(None),\n",
    "}\n",
    "\n",
    "# distances = {\n",
    "#     \"<0.25Mb\": slice(0,int(250_000/binsize10)+1),\n",
    "#     \"0.25-1Mb\": slice(int(500_000/binsize10),int(1_000_000/binsize10)+1),\n",
    "#     \"1-10Mb\": slice(int(1_000_000/binsize10),int(10_000_000/binsize10)+1),\n",
    "#     \">10Mb\": slice(int(10_000_000/binsize10),None),\n",
    "#     # \"50Mb-550Mb\": slice(int(50_000_000/binsize10),int(550_000_000/binsize10)+1),\n",
    "#     \"trans\": slice(None),\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(\n",
    "    nrows=len(sub_samples_m),\n",
    "    ncols=2*len(distances),\n",
    "    figsize=(4*len(distances),2*len(sub_samples_m)),\n",
    "    sharex=True,\n",
    "    sharey=True,\n",
    ")\n",
    "\n",
    "imshow_kwargs = dict(\n",
    "        norm=LogNorm(vmin=1/2, vmax=2),\n",
    "        cmap=\"RdBu_r\",\n",
    "        interpolation=\"none\",\n",
    ")\n",
    "\n",
    "for sample_m, sample_p, (i, axs) in zip(sub_samples_m, sub_samples_p, enumerate(axs)):\n",
    "    for jj, (_dist_name, _dist) in enumerate(distances.items()):\n",
    "        axm, axp = axs[jj], axs[len(distances) + jj]\n",
    "        if _dist_name != \"trans\":\n",
    "            Cm = np.nanmean(interaction_sums[sample_m][_dist], axis=0) / np.nanmean(interaction_counts[sample_m][_dist], axis=0)\n",
    "            Cp = np.nanmean(interaction_sums[sample_p][_dist], axis=0) / np.nanmean(interaction_counts[sample_p][_dist], axis=0)\n",
    "        elif _dist_name == \"trans\":\n",
    "            # pass\n",
    "            Cm = np.nanmean(interaction_sums_trans[sample_m][_dist], axis=0) / np.nanmean(interaction_counts_trans[sample_m][_dist], axis=0)\n",
    "            Cp = np.nanmean(interaction_sums_trans[sample_p][_dist], axis=0) / np.nanmean(interaction_counts_trans[sample_p][_dist], axis=0)\n",
    "        else:\n",
    "            pass\n",
    "        axm.imshow(Cm, **imshow_kwargs)\n",
    "        axp.imshow(Cp, **imshow_kwargs)\n",
    "        for _ax in [axp, axm]:\n",
    "            _ax.set_xticks([])\n",
    "            _ax.set_yticks([])\n",
    "        if i == 0:\n",
    "            axm.set_title(f\"m-{_dist_name}\")\n",
    "            axp.set_title(f\"p-{_dist_name}\")\n",
    "        if i == len(sub_samples_m)-1:\n",
    "            for _ax in [axm, axp]:\n",
    "                _ax.set_xticks(np.arange(len(ticklabels)))\n",
    "                _ax.set_xticklabels(np.asarray(ticklabels[::-1]), rotation=\"vertical\")\n",
    "        if jj == 0:\n",
    "            axm.set_ylabel(sample_m.lstrip(\"m\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(\n",
    "    nrows=len(sub_samples_m),\n",
    "    ncols=2*len(distances),\n",
    "    figsize=(4*len(distances),2*len(sub_samples_m)),\n",
    "    sharex=True,\n",
    "    sharey=True,\n",
    ")\n",
    "\n",
    "imshow_kwargs = dict(\n",
    "        norm=LogNorm(vmin=1/2.25, vmax=2.25),\n",
    "        cmap=\"RdBu_r\",\n",
    "        interpolation=\"none\",\n",
    ")\n",
    "\n",
    "for sample_m, sample_p, (i, axs) in zip(sub_samples_m, sub_samples_p, enumerate(axs)):\n",
    "    for jj, (_dist_name, _dist) in enumerate(distances.items()):\n",
    "        axm, axp = axs[jj], axs[len(distances) + jj]\n",
    "        if _dist_name != \"trans\":\n",
    "            Cm = np.nanmean(interaction_sums[sample_m][_dist], axis=0) / np.nanmean(interaction_counts[sample_m][_dist], axis=0)\n",
    "            Cp = np.nanmean(interaction_sums[sample_p][_dist], axis=0) / np.nanmean(interaction_counts[sample_p][_dist], axis=0)\n",
    "        elif _dist_name == \"trans\":\n",
    "            # pass\n",
    "            Cm = np.nanmean(interaction_sums_trans[sample_m][_dist], axis=0) / np.nanmean(interaction_counts_trans[sample_m][_dist], axis=0)\n",
    "            Cp = np.nanmean(interaction_sums_trans[sample_p][_dist], axis=0) / np.nanmean(interaction_counts_trans[sample_p][_dist], axis=0)\n",
    "        else:\n",
    "            pass\n",
    "        axm.imshow(Cm, **imshow_kwargs)\n",
    "        axp.imshow(Cp, **imshow_kwargs)\n",
    "        for _ax in [axp, axm]:\n",
    "            _ax.set_xticks([])\n",
    "            _ax.set_yticks([])\n",
    "        if i == 0:\n",
    "            axm.set_title(f\"m-{_dist_name}\")\n",
    "            axp.set_title(f\"p-{_dist_name}\")\n",
    "        if i == len(sub_samples_m)-1:\n",
    "            for _ax in [axm, axp]:\n",
    "                _ax.set_xticks(np.arange(len(ticklabels)))\n",
    "                _ax.set_xticklabels(np.asarray(ticklabels[::-1]), rotation=\"vertical\")\n",
    "        if jj == 0:\n",
    "            axm.set_ylabel(sample_m.lstrip(\"m\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # # the mix one - mp\n",
    "sub_samples_m = [\n",
    "    \"N93m5\",\n",
    "    \"N93m10\",\n",
    "]\n",
    "# p ...\n",
    "sub_samples_p = [\n",
    "    \"N93p5\",\n",
    "    \"N93p10\",\n",
    "]\n",
    "\n",
    "fig, axs = plt.subplots(\n",
    "    nrows=len(sub_samples_m),\n",
    "    ncols=2*len(distances),\n",
    "    figsize=(4*len(distances),2*len(sub_samples_m)),\n",
    "    sharex=True,\n",
    "    sharey=True,\n",
    ")\n",
    "\n",
    "imshow_kwargs = dict(\n",
    "        norm=LogNorm(vmin=1/2.5, vmax=2.5),\n",
    "        cmap=\"RdBu_r\",\n",
    "        interpolation=\"none\",\n",
    ")\n",
    "\n",
    "for sample_m, sample_p, (i, axs) in zip(sub_samples_m, sub_samples_p, enumerate(axs)):\n",
    "    for jj, (_dist_name, _dist) in enumerate(distances.items()):\n",
    "        axm, axp = axs[jj], axs[len(distances) + jj]\n",
    "        if _dist_name != \"trans\":\n",
    "            Cm = np.nanmean(interaction_sums[sample_m][_dist], axis=0) / np.nanmean(interaction_counts[sample_m][_dist], axis=0)\n",
    "            Cp = np.nanmean(interaction_sums[sample_p][_dist], axis=0) / np.nanmean(interaction_counts[sample_p][_dist], axis=0)\n",
    "        elif _dist_name == \"trans\":\n",
    "            Cm = np.nanmean(interaction_sums_trans[sample_m][_dist], axis=0) / np.nanmean(interaction_counts_trans[sample_m][_dist], axis=0)\n",
    "            Cp = np.nanmean(interaction_sums_trans[sample_p][_dist], axis=0) / np.nanmean(interaction_counts_trans[sample_p][_dist], axis=0)\n",
    "        else:\n",
    "            pass\n",
    "        axm.imshow(Cm, **imshow_kwargs)\n",
    "        axp.imshow(Cp, **imshow_kwargs)\n",
    "        for _ax in [axp, axm]:\n",
    "            _ax.set_xticks([])\n",
    "            _ax.set_yticks([])\n",
    "        if i == 0:\n",
    "            axm.set_title(f\"m-{_dist_name}\")\n",
    "            axp.set_title(f\"p-{_dist_name}\")\n",
    "        if i == len(sub_samples_m)-1:\n",
    "            for _ax in [axm, axp]:\n",
    "                _ax.set_xticks(np.arange(len(ticklabels)))\n",
    "                _ax.set_xticklabels(np.asarray(ticklabels[::-1]), rotation=\"vertical\")\n",
    "        if jj == 0:\n",
    "            axm.set_ylabel(sample_m.lstrip(\"m\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_samples_m =[\n",
    "        \"m10hR1R2\",\n",
    "        \"p10hR1R2\",\n",
    "        \"mp10hR1R2\",\n",
    "    ]\n",
    "sub_samples_p = [\n",
    "        \"N93m10\",\n",
    "        \"N93p10\",\n",
    "        \"N93mp10\",\n",
    "    ]\n",
    "\n",
    "fig, axs = plt.subplots(\n",
    "    nrows=len(sub_samples_m),\n",
    "    ncols=2*len(distances),\n",
    "    figsize=(4*len(distances),2*len(sub_samples_m)),\n",
    "    sharex=True,\n",
    "    sharey=True,\n",
    ")\n",
    "\n",
    "imshow_kwargs = dict(\n",
    "        norm=LogNorm(vmin=1/2.5, vmax=2.5),\n",
    "        cmap=\"RdBu_r\",\n",
    "        interpolation=\"none\",\n",
    ")\n",
    "\n",
    "for sample_m, sample_p, (i, axs) in zip(sub_samples_m, sub_samples_p, enumerate(axs)):\n",
    "    for jj, (_dist_name, _dist) in enumerate(distances.items()):\n",
    "        axm, axp = axs[jj], axs[len(distances) + jj]\n",
    "        if _dist_name != \"trans\":\n",
    "            Cm = np.nanmean(interaction_sums[sample_m][_dist], axis=0) / np.nanmean(interaction_counts[sample_m][_dist], axis=0)\n",
    "            Cp = np.nanmean(interaction_sums[sample_p][_dist], axis=0) / np.nanmean(interaction_counts[sample_p][_dist], axis=0)\n",
    "        elif _dist_name == \"trans\":\n",
    "            Cm = np.nanmean(interaction_sums_trans[sample_m][_dist], axis=0) / np.nanmean(interaction_counts_trans[sample_m][_dist], axis=0)\n",
    "            Cp = np.nanmean(interaction_sums_trans[sample_p][_dist], axis=0) / np.nanmean(interaction_counts_trans[sample_p][_dist], axis=0)\n",
    "        else:\n",
    "            pass\n",
    "        axm.imshow(Cm, **imshow_kwargs)\n",
    "        axp.imshow(Cp, **imshow_kwargs)\n",
    "        for _ax in [axp, axm]:\n",
    "            _ax.set_xticks([])\n",
    "            _ax.set_yticks([])\n",
    "        if i == 0:\n",
    "            axm.set_title(f\"m-{_dist_name}\")\n",
    "            axp.set_title(f\"p-{_dist_name}\")\n",
    "        if i == len(sub_samples_m)-1:\n",
    "            for _ax in [axm, axp]:\n",
    "                _ax.set_xticks(np.arange(len(ticklabels)))\n",
    "                _ax.set_xticklabels(np.asarray(ticklabels[::-1]), rotation=\"vertical\")\n",
    "        if jj == 0:\n",
    "            axm.set_ylabel(sample_m)\n",
    "\n",
    "\n",
    "\n",
    "# try adding an axes manually ...\n",
    "cax = fig.add_axes([0.88,0.001,0.1,0.02])\n",
    "fig.colorbar(\n",
    "    cm.ScalarMappable(norm=imshow_kwargs[\"norm\"], cmap=imshow_kwargs[\"cmap\"]),\n",
    "    cax=cax,\n",
    "    orientation=\"horizontal\",\n",
    ")\n",
    "cax.set_xticks([imshow_kwargs[\"norm\"].vmin, 1, imshow_kwargs[\"norm\"].vmax])\n",
    "cax.set_xticklabels([imshow_kwargs[\"norm\"].vmin, 1, imshow_kwargs[\"norm\"].vmax])\n",
    "cax.minorticks_off()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
